* Scientists spend tons of time preparing data
* Especially valid for annotation problems
* Different datasets in different formats
* Heterogeneous landscape of tools and measures as well
* Comparability of experiments not given
* Archiving problematic
* Reproducibility thus not given as well
* Central repository for experimental results not given
* Integration/publication of results difficult
* End users do not know what to use for their use case
* Diagnostics for developers (why do I perform poorly on a dataset where others perform well)

=> Solution: Community-driven effort for the evaluation of annotation tools, open-source framework (extensible and customizable), publishable and machine-readable result format, stable and publishable URIs for experimental results, comparability and reproducibility are thus ensured: GERBIL

=> Table with datasets and their formats, experiment type, matching type, tools and their formats

* Overview
* Components
	* Experiment type
	* Matching type
	* Annotators (	1. How to add to Gerbil; 2. Current annotators; 3. Sota/future work)
	* Datasets (License)
	* Measures
* Use Cases
* Evaluation: Costs (time) for integrating tools into GERBIL
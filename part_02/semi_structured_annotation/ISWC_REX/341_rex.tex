
%\documentclass{llncs}

%\begin{document}
 

\chapter{Web-Scale Extension of RDF Knowledge Bases from Templated Websites}

%\author{
%Lorenz B\"uhmann\thanks{Both authors contributed equally to this work.}\inst{1} \and 
%Ricardo Usbeck$^\star$\inst{1}\inst{2}\and
%Axel-Cyrille Ngonga Ngomo\inst{1} \and 
%Muhammad Saleem \inst{1} \and
%Andreas Both \inst{2}\and
%Valter Crescenzi\inst{3} \and       
%Paolo Merialdo\inst{3}\and
%Disheng Qiu\inst{3}
%}
%\institute{Universit\"at Leipzig, IFI/AKSW  
%\email{\{lastname\}@informatik.uni-leipzig.de} \and 
%Unister GmbH, Leipzig
%\email{\{firstname.lastname\}@unister.de}\and
%Universit\`a Roma Tre
%\email{\{lastname\}@dia.uniroma3.it}
%}

%\date{8 October 2013}

%\maketitle
%\begin{abstract}
%Over the last years, Linked Open Data has been utilized in several scientific and industrial information systems. 
%Despite the significant growth of the Linked Open Data Cloud over the last years, 
Only a small fraction of the information on the Web is represented as Linked Data.
This lack of coverage is partly due to the paradigms followed so far to extract Linked Data. 
While converting structured data to RDF is well supported by tools, most approaches to extract RDF from semi-structured data rely on extraction methods based on ad-hoc solutions. 
%Existing wrapper induction approaches for RDF extraction do not ensure the consistency of the knowledge they extract due to the lack of expressive schemas on the Linked Open Data Cloud. 
%Consequently, the extraction of consistent RDF data from web sources remains an unsolved problem. 
In this paper, we present a holistic and open-source framework for the extraction of RDF from templated websites.
%learning web wrappers using knowledge from the Linked Open Data Cloud.
We discuss the architecture of the framework and the initial implementation of each of its components.
In particular, we present a novel wrapper induction technique that does not require any human supervision to detect wrappers for web sites. 
Our framework also includes a consistency layer with which the data extracted by the wrappers can be checked for logical consistency.
%Moreover, our approach achieves superior F-measures than the state of the art.
We evaluate the initial version of REX on three different datasets. 
%We study the effect of the number of examples on the F-measure achieved by our approach. 
Our results clearly show the potential of using templated Web pages to extend the Linked Data Cloud.
Moreover, our results indicate the weaknesses of our current implementations and how they can be extended.
%\end{abstract}

%\begin{itemize}
%\item Describe the framework (made for generic purpose)
%\item how and with which modules has the framework been initialized
%\item create javadoc
%\item clean source code
%\end{itemize}

%\todo[inline]{
%
%Software Frameworks advance science by sharing with the community software that can easily be extended or adapted to support scientific study and experimentation. Jena, Sesame, hadoop, and many other software frameworks have clearly impacted our community but were, similarly, difficult to publish.  For example:
%
%Brian McBride. Jena: A Semantic Web Toolkit. IEEE Internet Computing. November, 2002. [PDF]
%
%Review Criteria: Does the framework solve a problem that is useful to the ISWC community?  Are there similar frameworks? How general is the framework in terms of applicability? What is the novelty of the abstractions chosen and are they useful? What can it not do and what is the rationale for the exclusion of that functionality? Does it use open standards, when applicable, or have good reason not to?  Is it open-source, freely available, extendable? How applicable is it to a wide variety of problems?  
%
%We encourage the authors to carefully read the calls for the other tracks, the Research track, the  In Use track and the Industry track and consider submitting to the most 
%appropriate track. Multiple submissions of the same paper to different tracks are not acceptable.}
\section{Introduction}

The Linked Open Data (LOD) Cloud has grown from 12 datasets (also called knowledge bases) to over 2000 knowledge bases in less than 10 years.\footnote{\url{http://stats.lod2.eu/}}
%The amount of data now available on the LOD Cloud exceeds 31 billion triples, which pertain to domains as diverse as movies, music, sports, bio-medicine and many others. 
This steady growth of the LOD Cloud promises to continue as very large datasets such as  Linked TCGA~\cite{SAL+13a} with 20.4 billion triples are added to it. However, the LOD Cloud still contains only a fraction of the knowledge available on the Web~\cite{GER+13}. This lack of coverage is mainly due to the way the data available on the LOD Cloud is extracted. Most commonly, the data in the LOD Cloud originates from one of two types of sources: structured data (especially databases such as Drugbank,\footnote{\url{http://www.drugbank.ca}} Diseasome,\footnote{\url{http://diseasome.eu}} etc.)~and semi-structured data sources (for example data extracted from the Wikipedia\footnote{\url{http://wikipedia.org}} infoboxes). While generating RDF triples from structured data (especially databases) is well supported by tools such as Triplify~\cite{DBLP:dblp_conf/www/AuerDLHA09}, D2R~\cite{Bizer04} and SPARQLMap~\cite{DBLP:conf/aswc/UnbehauenSA12}
, devising automatic means to generate RDF from semi-structured data is a more challenging  problem. 
Currently, this challenge is addressed by ad-hoc or manual (e.g., community-driven) solutions. 
For example, the well-known DBpedia~\cite{DBLP:conf/semweb/AuerBKLCI07} provides a mapping Wiki\footnote{\url{http://mappings.dbpedia.org}} where users can explicate how the content of infoboxes is to be transformed into RDF. On the one hand, manual approaches offer the advantage of leading to high-precision data; on the other hand, they suffer of a limited recall because of the small number of web sources from which the data is extracted. For example, DBpedia only contains a fraction of the movies that were published over the last years because it was extracted exclusively from Wikipedia.
%\footnote{\url{http://wikipedia.org}} 
Moreover, the same knowledge base only contains a fraction of the cast of some of the movies it describes.

The main aim of this paper is to address the challenge of extracting RDF from semi-structured data.
We introduce REX, an open-source framework for the extraction of RDF from templated websites (e.g., Wikipedia, IMDB, ESPN, etc.).
%\todo[inline]{From the rebuttal: mention the API they have, and that we are aware of them}
%With REX which we aim to facilitate reduce the gap between the content of the document Web and the content of the Data Web
REX addresses the extraction of RDF from templated websites by providing a modular and extensible architecture for learning XPath wrappers and extracting consistent RDF data from these web pages.
Our framework is thus complementary to RDF extraction frameworks for structured and unstructured data.
While REX targets the extraction of RDF from templated websites in its current version, the architecture of the framework is generic and allows for creating versions of the tool that can extract RDF from other sources on websites, for example from unstructured data or from the billions of tables available on the Web.
Our framework has the following features:
\begin{enumerate}
\item \emph{Extensibility}, i.e., our framework is open-source, available under the MIT license and can thus be extended and used by any third party;
\item \emph{Use of standards}, i.e., REX relies internally on widely used libraries and on W3C Standards such as RDF, SPARQL and OWL;
\item \emph{Modularity}, i.e., each of the modules can be replaced by another implementation;
\item \emph{Scalability}, i.e., the current algorithms can be used on large amounts of data; 
\item \emph{Low costs}, i.e., REX requires no human supervision; 
\item \emph{Accuracy}, i.e., the current implementation achieves satisfactory F-measures 
%while extracting relevant information contained in the input websites 
and
\item \emph{Consistency}, i.e., REX implements means to generate triples which abide by the ontology of the source knowledge base providing the training data.
\end{enumerate}

%\todo[inline]{RU: The section below is hard to understand since it comprises too much information. How about a dataflow diagram to make it more comprehensive? AN: No space for that. Architecture diagram does the job later. Simplified the section.}
In addition to being novel in itself, REX introduces a \emph{novel wrapper induction technique} for extracting structured data from templated Web sites. 
This induction approach makes use of the large amount of data available in the LOD Cloud as training data. 
By these means, REX circumvents the problem of high annotation costs faced by several of the previous wrapper induction approaches~\cite{Hogue:2005:TAU:1060745.1060762,flesca2004web} while keeping the high accuracy of supervised wrapper induction methods. 
%The input for REX's RDF extraction is a set of subject-object pairs $(s, o)$ from an input knowledge base $K$ which are all connected by a given predicate $p$ and a set of web pages. 
%Based on the set of pairs, REX learns web wrappers for the extraction of novel RDF data from the input web pages. 
By post-processing the output of website wrappers, our system can generate novel triples. 
To ensure that these novel triples are consistent, REX provides a consistency check module which computes and uses the axioms which underlie the input knowledge base $K$. 
Only those triples which do not break the consistency rules are returned by REX. 
The contributions of this paper are consequently as follows:
\begin{enumerate}
\item We introduce a novel framework for the extraction of RDF from templated websites.
\item We present a novel wrapper induction approach for the extraction of subject-object pairs from the Web.
\item We integrate state-of-the-art disambiguation and schema induction techniques to retrieve high-quality RDF. 
\item We evaluate the first version of REX on three datasets and present both the strengths and weaknesses of our approach.
\item Overall, we present the (to the best of our knowledge) first web-scale, low-cost, accurate and consistent framework that allows extracting RDF from structured websites.
\end{enumerate}

The rest of this paper is organized as follows. In Section~\ref{sec:notation}, we introduce the notation that underlies this paper and the problems that we tackle. Section~\ref{sec:rex} presents the architecture of REX in more detail as well as the current implementation of each of its components.
In particular, we illustrate our approach to generate examples from a knowledge base $K$ and we show our algorithm to learn web wrappers from such examples.
Subsequently, we give an overview of AGDISTIS~\cite{Usbeck2014} which we use to address the problem of URI disambiguation. 
Finally, we describe our current solution to ensuring the validity of the data generated by REX. 
In Section~\ref{sec:evaluation} we present the results of REX on 3 datasets, each containing at least 10,000 pages. 
We discuss related work in Section~\ref{sec:related}, and we conclude the paper in Section~\ref{sec:conclusions}. 
More information on REX can be found at~\url{http://aksw.org/Projects/REX} including inks to the source code repository (incl. examples), to the documentation and to a tutorial of the framework.

\section{Notation and Problem Statement}
\label{sec:notation}
In this section, we present the concepts and notation to understand the concept behind REX. We denote RDF triples as $<s, p, o>$ where $(s, p, o) \in R \times P \times (R \cup L)$. We call $R$ the set of resources, $P$ the set of properties and $L$ the set of literals. We call $A = R \cup P \cup L$ the set of all atoms. We regard knowledge bases $K$ as sets of triples. We denote the set of all pairs $(s, o)$ such that $<s, p, o> \in K$ with $pairs(p, K)$.
We define the in-degree $in(a)$ of an atom $a$ in $K$ as the number of distinct $x$ such that there is a predicate $q$ with $<x, q, a> \in K$. Conversely, the out-degree $out(a)$ of $a$ is defined as the number of distinct atoms $y$ that are such that there exists a predicate $q'$ with $<a, q', y> \in K$.
We assume the existence of a labeling function $\labf$, which maps each element of $A$ to a sequence of words from a dictionary $D$. Formally, $\labf: A \rightarrow 2^D$. For example, the value of $\labf$(\texttt{r}) can be defined as the set of \texttt{x} with $<$\texttt{r, rdfs:label, x}$> \in K$ if \texttt{r} is a resource and as the lexical form of  \texttt{r} if  \texttt{r} is a literal.
%\todo[inline]{RU: define rdfs? AN: Not necessary}
%\footnote{For more information on RDF and the concepts all around this language, please consult the W3C RDF specification at \url{http://www.w3.org/RDF/}. \texttt{rdfs} stands for \url{http://www.w3.org/2000/01/rdf-schema#}.} A dictionary $D$ can simply be the set of all expressions of a particular natural language such as English, German or Italian. 

Based on this formalisation, we can define the problem that REX addresses as follows: Given (1)
%\begin{enumerate}
%\item 
a predicate $p$ that is contained in a knowledge base $K$ 
%\item 
and (2) a set of unlabeled web pages $W = \{w_1, w_2, \ldots, w_{|W|}\}$,
%\end{enumerate}
extract a set of triples $<s_i, p, o_i>$ from the websites of $W$.
Several tasks have to be addressed and solved to achieve this goal within the paradigm that we adopt: %machine-learning paradigm automatically supervised with knowledge from the LOD Cloud that we adopt.

%\todo[inline]{Maybe we need to introduce a consistency function C so Problem 1 can be written as ...set of pairs $(s, o)$ for which $C(<s, p, o>)$ is false.} 
%\paragraph{Problem 1}
%First, two sets of examples are required:
%\begin{enumerate}
%\item A set $P$ of positive examples for a predicate $p$, i.e., a set of pairs $(s, o)$ for which $<s, p, o>$ is true and
%\item A set $N$ of negative examples for a predicate $p$, i.e., a set of pairs $(s, o)$ for which $<s, p, o>$ is false.
%\end{enumerate}
%We propose several approaches for the generation of negative examples from triples in a knowledge base $K$ and study the influence of these methods on the accuracy of our system.

\paragraph{Problem 1:}
%\todo[inline]{Disheng: Probably we should talk about wrappers and that they are often described as XPaths? Axel: Done}
%Given a set of examples ${\examples}$ for the predicate $p$ from a knowledge base $K$, ${\examples} = \{(s,o):\ <s,p,o> \in K\}$
We first require an approach for extracting pairs of resource labels out of unlabelled pages $w_i$. 
We tackle this problem by means of a wrapper induction algorithm (see~Section~\ref{alfred}). 
We assume that we are given (1) a set ${\examples} \subseteq \{(s,o):\ <s,p,o> \in K\}$ of positive examples for a predicate $p$ from the Linked Data Web and (2) a set of web pages $W$ without any labeling. 
Our aim is to generate high-quality wrappers, expressed as pairs of XPath expressions over these unlabeled web pages $W$, that extract a pair of values from each page.
%Note that this is the core task to address the problem.
%\todo[inline]{WR at hand. Axel: What does this mean?}
%\todo[inline]{RU: leave out the distinction between positive and negative examples? so reviewers are not confused. AN: Cannot find a mention of negative examples }
\paragraph{Problem 2:}
Once the  pairs of values have been extracted from web pages, we need to ground them in the knowledge base $K$. 
In this context, grounding means that for each value extracted by our solution to Problem 1 we have to either (1) find a matching resource or (2) generate a novel resource or literal for this particular value. 
We address this challenge by using a URI disambiguation approach that combines breadth-first search and graph algorithms to determine a resource that matches a given string. 
If no URI is found, our approach generates a new resource URI (see~Section~\ref{urigen}).

\paragraph{Problem 3:}
Once new knowledge has been generated, it is central to ensure that the knowledge base $K$ to which it is added remains consistent. 
To this end, we need to ensure that we do not add any statements to $K$ that go against its underlying axioms. 
The problem here is that these axioms are not always explicated in knowledge bases in the LOD Cloud. 
We thus devise an approach to generate such axioms from instance data (see~Section~\ref{axioms}). 
To achieve this goal, we use a statistical analysis of the use of predicates across the knowledge base $K$. 
Moreover, we provide means to use RDFS inference to generate new knowledge from new resources generated by our solution to Problem 2.


%Add examples to clarify the problem 
\section{The REX Framework}
\label{sec:rex}
%\todo[inline]{State that we use DBpedia as underlying KB}
In the following, we present REX, an integrated  solution to the three problems presented above.
We begin by giving an overview of its architecture.
Then, we present each of its components.
As running example, we use the extraction of movie directors from web pages.

\subsection{Overview}
Figure~\ref{charex:fig:architecture} gives an overview of REX. 
All modules are interfaces, for which we provide at least one implementation.
Hence, REX can be ran out of the box.
Given a predicate $p$ and a knowledge base $K$, REX provides a domain identification interface, which allows for detecting Web domains which pertain to this predicate.
For example, the predicate \texttt{dbo:actor} leads to the domain \url{http://imdb.com} being retrieved.
From this domain,  a set $W$ of web pages can be retrieved by using a \emph{crawler}.
The results of the crawling are stored in a solution for unstructured data, for example an index. 
REX then generates a set of examples using an instantiation of the \emph{example generator} interface. 
The goal here is to generate a sample $\examples$ of all elements of $pairs(p, K)$ that allows learning high-quality pairs of XPath expressions. 
The examples are given to a \emph{wrapper inducer}, which learns pairs of XPath expressions for extracting the pairs of values in $\examples$ from the elements of $W$. 
These pairs are then applied to all pages of $W$.
The extraction results, i.e., pairs of strings, are passed on to a \emph{URI generator}, which implements a graph-based disambiguation approach for finding or generating URIs for the strings contained in the extraction results.
The resulting set $C$ of candidate triples are finally forwarded to an \emph{validation engine}, which learns axioms from $K$ and applies these to $C$ to derive a set of triples that are consistent with $K$. 
In the following, we detail our current implementation of each of these components.  

\begin{figure}[htb]
\centering
\includegraphics[width = \textwidth]{part_02/semi_structured_annotation/ISWC_REX/rexArchitecture}
\caption{Architecture of REX.}
\label{charex:fig:architecture}
\end{figure}
\subsection{Extraction Layer}
REX's data extraction layer consists of two main components:
The \emph{domain identification module} is the first component of the layer and takes a set of triples $(s, p, o)$ as examples and returns a ranked list of Web domains.
Our current implementation simply uses the Google interface to search for websites that contain the label of all $s$, $p$ and $o$.
The top-10 domains for each triple are selected and their rank is averaged over all triples.
The resulting ranking is returned. 
For our example \texttt{dbo:actor}, we get \url{http://imdb.com} as top-ranked domain.
The second component consists of a \emph{crawler interface} which allows to gather the web pages that are part of the detected domain and collect them in a storage solution for unstructured data.
Currently, we rely on crawler4j\footnote{\url{https://code.google.com/p/crawler4j/}} for crawling and Apache Lucene\footnote{\url{http://lucene.apache.org/}} for storing the results of the crawling.  

\subsection{Storage Layer}
The storage layer encapsulates the storage solutions for structured data (i.e., the knowledge base $K$) and the unstructured data (i.e., the output of the extraction layer). 
We assume that the structured data can be access via SPARQL.
The unstructured data storage is expected to return data when presented with a pair $(s, o)$ of resources, which is commonly a positive or negative example for the pairs that abide by $p$.
As stated above, we rely on a Lucene index that can access the labels of resources and simply search through its index for pages that contain both a label for $s$ and a label for $o$.

\subsection{Induction Layer}
The induction layer uses the data in the storage layer to compute wrappers for the website crawled in the first step.
To this end, it contains two types of modules:
The \emph{example generation} module implements sampling algorithms that are used to retrieve examples of relevant pairs $(s, o)$ such that $(s, p, o) \in K$. 
These examples are used to feed the \emph{wrapper induction} module, which learns the wrappers that are finally used to extract data from web pages.
Hereafter, we present the implementations of these modules.

\subsubsection{Generation of Examples}
Given a knowledge base $K$, the generation of all examples $\examples$ for a predicate $p$ can be retrieved by computing all triples $<s,p,o>$ from $K$.
However, using all triples might lead to poor scalability, especially if $K$ is very large.
To ensure the scalability of our approach, we thus aimed to ensure that we can provide REX with only a sample of $\examples$ and thus reduce its learning runtime without diminishing its accuracy.  
Our first intuition was that it is more likely to find resources that stand for well-known real-world entities on the Web. 
Thus, by selecting the most prominent examples from the knowledge $K$, we should be able to improve the probability of finding web pages that contain both the subject and the object of our examples. 
This intuition can be regarded as prominence-driven, as it tries to maximize the number of annotated pages used for learning. 
We implemented this intuition to generating a sample of $\examples$ by implementing a first version of the example generator that ranks the examples $(s, o)$ in $\examples$ in descending order by how prominent they are in the knowledge base. The score $scr$ for ranking the examples was computed by summing up the in- and out-degree of $s$ and $o$:
$scr(s, o) = in(s) + in(o) + out(s) + out(o)$. 
We call this example selection \emph{prominence-based}.

The main drawback of this first intuition is that it introduces a skew in the sampling as we only consider a subset of entities with a particular distribution across the pages in $W$. 
For example, actors in IMDB have different templates depending on how popular they are. 
Learning only from popular actors would then lead to learning how to extract values only from web pages obeying to particular type of HTML template. 
%Thus, as our experiments show, while this approach achieves a high recall when looking for web pages, it can lead to incorrect wrappers if only a small subset of $P$ is selected for learning.
%maybe we should write here that the correction of this bias is carried out by the learning appraoch?
While this problem can be by choosing a large number of examples, we revised our sampling approach to still use the ranking but to sample evenly across the whole list of ranked resources. 
To this end, given a number $n$ of required pairs, we return the first $n$ pairs $(s,o)$ from the ranked list computed above whose index $idx$ abides by
$idx(s, o) \equiv 0  \left(mod\left \lfloor{\frac{|\examples|}{n}}\right \rfloor \right).$
We call this second implementation of the example generator interface the \emph{uniform} approach .
%By these means, we can ensure that the whole of the distribution of resource prominence is covered by our approach. Consequently, we can support the generation of robust wrappers from the given input data.

%\paragraph{Negative Examples}
%Although it's sufficient to restrict to positive examples only, the usage of negative examples can help to improve the \emph{wrapper induction} step by reducing the hypothesis space. Assuming that we have a positive example $<s,p,o>$ and $dom(p)$(resp. $ran(p)$) denotes the domain (resp. range) of $p$, we generate negative examples as follows:
%\begin{itemize}
%\item Generate a new triple $<s',p,o>$ where $s'$ is an instance of $dom(p)$ and $<s',p,o>$ does not exist in $K$.
%\item Generate a new triple $<s,p,o'>$ where $o'$ is an instance of $ran(p)$ and $<s,p,o'>$ does not exist in $K$.
%\item Generate a new triple $<s',p,o'>$ where $s'$ is an instance of $dom(p)$, $o'$ is an instance of $ran(p)$ and $<s',p,o'>$ does not exist in $K$.
%\end{itemize}


\subsubsection{Wrapper Generation} %(DQ, VC, PM)
\label{alfred}
%- Several approaches in the literature to generate wrappers
%Several years of reasearch on data extraction from web pages have produced a large number of proposals for producing web wrappers (see~\cite{} for a survey). 

\input{part_02/semi_structured_annotation/ISWC_REX/alfred}

\subsection{Generation Layer}
Now that data has been extracted from the websites, REX is ready to generate RDF out of them. 
To achieve this goal, two steps needs to be carried out. 
First, the strings retrieved have to be mapped to RDF resources or literals. 
This is carried out by the \emph{URI disambiguation} modules. 
The resulting triples then need to be checked for whether they go against the ontology of the knowledge base or other consistency rules. 
This functionality is implemented in the \emph{data validation} modules. 

\subsubsection{URI Disambiguation}\label{urigen}
%Subsequently to generating the best possible wrapper for a domain of a property $p$ we need to generate triples from each pair $(s, o) \in T$.
%To achieve this goal, we try to match $(s,o)$ against resources from the knowledge base $K$.
URI disambiguation is not a trivial task, as several resources can share the same label in a knowledge base. 
For example, ``Brad Pitt'' can be mapped to the resource \texttt{:Brad\_Pitt} (the movie star) or \texttt{:Brad\_Pitt\_(boxer)}, an Australian boxer. 
We address this problem by using \emph{AGDISTIS}, a framework for URI disambiguation~\cite{Usbeck2014}.
In our current implementation, we chose to simply integrate the AGDISTIS framework using DBpedia 3.8.
We chose this framework because it outperforms the state-of-the-art frameworks AIDA~\cite{AIDA} and DBpedia Spotlight~\cite{spotlight} by 20\% w.r.t. its accuracy. 
Especially on short RSS feeds containing only two resource labels, the approach achieves 3\%  to 11\% higher accuracy. 
More details on AGDISTIS as well as a thorough evaluation against popular frameworks such as DBpedia Spotlight and AIDA can be found in~\cite{Usbeck2014}.
Note that if no resources in $K$ has a URI which matches $s$ or $o$, we generate a new cool URI\footnote{\url{http://www.w3.org/TR/cooluris}} for this string.% following the DBpedia guidelines~\cite{DBLP:conf/semweb/AuerBKLCI07}.

\subsubsection{Data Validation}\label{axioms}
Sequentially applying the steps before results in a set of triples $<s,p,o>$ that might not be contained in $K$. 
As we assume that we start from a consistent knowledge base $K$ and the whole triple generation process until here is carried out automatically, we need to ensure that $K$ remains consistent after adding $<s,p,o>$ to $K$.
To this end, REX provides a data validation interface whose first implementation was based on the DL-Learner.\footnote{\url{http://dl-learner.org}}
Depending on the size of $K$, using a standard OWL reasoner for consistency checks can be intractable.
Thus, our current implementation applies the following set of rules based on the schema of $K$ and add a triple $<s_1,p,o_1>$ only if it holds that:
\begin{enumerate}
\item If a class $C$ is the domain of $p$, there exists no type $D$ of $s_1$ such that $C$ and $D$ are disjoint.
\item If a class $C$ is the range of $p$, there exists no type $D$ of $o_1$ such that $C$ and $D$ are disjoint.
\item If $p$ is declared to be functional, there exists no triple $<s_1,p,o_2>$ in $K$ such that $o_1 \neq o_2$.
\item If $p$ is declared to be inverse functional, there exists no triple $<s_2,p,o_1>$ in $K$ such that $s_1 \neq s_2$.
\item If $p$ is declared to be asymmetric, there exists no triple $<o_1,p,s_1>$ in $K$.
\item If $p$ is declared to be irreflexive, it holds that $s_1 \neq o_1$.
\end{enumerate}
Note that this approach is sound but of course incomplete.
Although an increasing number of RDF knowledge bases are published, many of those consist primarily of instance data and lack sophisticated schemata. 
To support the application of the above defined rules, we follow the work in \cite{Buehmann2012,pattern_enrichment}, which provides a lightweight and efficient schema creation approach that scales to large knowledge bases. 
%The creation of the (extended) schema is done in three steps:

%\begin{enumerate}
%\item In the optional first step, SPARQL queries are used to obtain existing information about the schema of the knowledge base, in particular we retrieve axioms which allow to construct the class hierarchy. Naturally, the schema only needs to be obtained once per knowledge base and can then be re-used by all algorithms and all entities.
%\item The second step consists of obtaining all the data via SPARQL, which is relevant for learning the considered axiom.
%This results in a set of axiom candidates.
%\item In the third step, the score of axiom candidates is computed and the results returned.
%\end{enumerate}

%Suppose we want to search for the domain of $p$ and got a class $A$ as possible candidate from step 2, then we have to run 3 SPARQL queries to obtain all data for computing the score by means of precision and recall: one query for the number of instances having at least one $p$ ($|\exists p.\top|$), one for the instance count of $A$ ($|A|)$, and another one to get the number of instances contained in the intersection of both ($|\exists p.\top \sqcap A|$).
%Based on that information, we can compute precision $P$ as $P=\frac{|\exists p.\top \sqcap A|}{|A|}$ and recall $R$ as $R=\frac{|\exists p.\top \sqcap A|}{|\exists p.\top|}$, both resulting in a total score for  $A$  being the domain of $p$ using standard F-measure.

%A disadvantage of using this straightforward method of obtaining a score is that it does not take the \emph{support} for an axiom in the knowledge base into account. 
%Specifically, there would be no difference between having 100 out of 100 correct observations or 3 out of 3 correct observations when computing precision and recall.
%For this reason, we do not just consider the count, but the average of the 95\% confidence interval of the count.
%This confidence interval can be computed efficiently by using the improved Wald method defined in~\cite{approx}. 
%Assume we have $\mu$ observations out of which $\sigma$ were successful, then the approximation of the 95\% confidence interval is as follows:

%\begin{scriptsize}
%\[
%\left[
% \max(0, p' - 1.96 \cdot \sqrt{\frac{p' \cdot (1-p')}{\mu+4}}) , %\textnormal{ to }
% \min(1, p' + 1.96 \cdot \sqrt{\frac{p' \cdot (1-p')}{\mu+4}}) 
%\right]
%\]
%\end{scriptsize}

%\noindent
%with $p' = \frac{\sigma+2}{\mu+4}$.
%This formula is easy to compute and has been shown to be accurate in~\cite{approx}.
%This step concludes the extraction of RDF triples from websites.

\section{Evaluation}
\label{sec:evaluation}
The goal of the evaluation was to provide a detailed study of the behavior of the current REX modules with the aim of (1) ensuring that our framework can be used even in its current version and (2) detecting current weaknesses of our framework to trigger future developments.
In the following, we begin by presenting the data and hardware we used for our experiments. 
Thereafter, we present and discuss the results of our experiments.
Detailed results can be found at the project website.

%\begin{enumerate}

%\item We wanted to determine the amount of knowledge that we need to learn sensible pairs of XPath expressions. We thus provided our wrapper inference component with a variable number of examples and measured the influence of the number of examples on the overall F-measure of our approach. 
%%\item We also wanted to evaluate how well our approach can estimate its own confidence in the expressions it learned. We thus conducted a series of experiments where we measured the change in sure of our approach with varying confidence thresholds.
%\item To measure the runtime performance of our approach, we measured the amount of time that REX required to learn  wrappers from the input data. Especially, we measures the influence of the size of $I$ on the runtime of REX as well as on the average F-measure achieved by our approach. 
%\item Finally, we wanted to measure the quality of the end result of our approach, i.e., the quality of the output RDF. We thus sampled the triples generated by our approach and approximated its overall accuracy. 
%\end{enumerate}
 
\subsection{Experimental Setup}
We generated our experimental data by crawling three websites, i.e., 
\begin{enumerate}
\item \url{imdb.com} where we extracted \url{dbo:starring}, \url{dbo:starring}$^{-1}$ and \url{dbo:director};
\item \url{goodreads.com}, from which we extracted \url{dbo:author} and \url{dbo:author}$^{-1}$;
\item \url{espnfc.com} with the target relations \url{dbo:team} and \url{dbo:team}$^{-1}$.
\end{enumerate}
%For each website, we focused on two subdomains (see~Table~\ref{tab:dataset}). 
We chose these websites because they represent three different categories of templated websites.
\url{imdb.com} widely follows a uniform template for all pages in the same subdomain.
%Moreover, there are rarely any pages with missing values in this site.
Thus, we expected the wrapper learning  to work well here.
\url{goodreads.com} represents an average case of templated websites. 
While template are most widely used and followed, missing values and misused fields are more common here than in our first dataset.
%Thus, we assume that the values achieved on this dataset can be regarded as prototypical for our approach.
The third dataset, \url{espnfc.com}, was chosen as worst-case scenario.
The dataset contains several blank pages, a large variety of  templates used in manifold different fashions.
Consequently, defining a set of golden XPaths is a tedious task, even for trained experts.
Thus, we expected the results on this dataset to be typical for the worst-case behavior of our approach.
%Since all domains support incremental URLs we randomly sampled valid numbers in the respective range of valid URLs. 
We randomly sampled 10,000 HTML pages per subdomain for our experiments and manually built reference XPath expressions to evaluate the precision and recall of the generated extraction rules. 
The precision, recall and F-measure reported below were computed by comparing the output of REX with the output of the reference XPath expressions.
All extraction runtime experiments were carried out on single nodes of an Amazon EC2.small instance.

%\todo[inline]{State that some of the used properties are uni-directional}
%\todo[inline]{In the paper of \cite{Crescenzi2013} a much more sophisticated crawling strategy is described. We could engage this by stating that we crawl a much higher amount of data without loosing web-scalability}


%\begin{table}[htb]
%\centering
%\caption{Properties per dataset. \texttt{dbo} stands for \texttt{http://dbpedia.org/ontology/}.} 
%\label{tab:dataset}
%\begin{tabular}{lll}
%\toprule
%%\multicolumn{2}{c}{Domain}&Property\\
%%\midrule
%\multirow{2}{*}{\url{goodreads.com}} & Author   & \url{dbo:author}$^{-1}$\\
%                                     & Book     & \url{dbo:author} \\
%\midrule
%\multirow{2}{*}{\url{espnfc.com}}    & Team     & \url{dbo:team}$^{-1}$\\
%                                     & Player   & \url{dbo:team} \\
%\midrule
%\multirow{2}{*}{\url{imdb.com}}      & Actor    & \url{dbo:starring}$^{-1}$ \\
%                                     & Movie    & \url{dbo:starring} \\
%                                      & Movie    & \url{dbo:director} \\
%\bottomrule
%\end{tabular}
%\end{table}

\subsection{Results}
\subsubsection{Effect of Number of Examples and Sampling Strategy on F-measure}

\begin{figure*}[h!tb]
        \centering
        \subfloat[Directors from IMDB, prominence-based sampling]{\label{charex:fig:pDirector}\includegraphics[width=.33\textwidth]{part_02/semi_structured_annotation/ISWC_REX/no-random-director.pdf}}~
        \subfloat[Actors from IMDB, prominence-based sampling]{\label{charex:fig:pActors}\includegraphics[width=.33\textwidth]{part_02/semi_structured_annotation/ISWC_REX/no-random-starring.pdf}}~
        \subfloat[Authors from Goodreads, prominence-based sampling]{\label{charex:fig:pAuthor}\includegraphics[width=.33\textwidth]{part_02/semi_structured_annotation/ISWC_REX/no-random-author.pdf}}

        \subfloat[Directors from IMDB, uniform sampling]{\label{charex:fig:uDirector}\includegraphics[width=.33\textwidth]{part_02/semi_structured_annotation/ISWC_REX/random-director.pdf}}~
        \subfloat[Actors from IMDB, uniform sampling]{\label{charex:fig:uActors}\includegraphics[width=.33\textwidth]{part_02/semi_structured_annotation/ISWC_REX/random-starring.pdf}}~
        \subfloat[Authors from Goodreads, uniform sampling]{\label{charex:fig:uAuthor}\includegraphics[width=.33\textwidth]{part_02/semi_structured_annotation/ISWC_REX/random-author.pdf}}
        
        \subfloat[Teams from ESPNFC, prominence-based sampling]{\label{charex:fig:pTeam}\includegraphics[width=.33\textwidth]{part_02/semi_structured_annotation/ISWC_REX/no-random-team.pdf}}~
        \subfloat[Teams from ESPNFC, uniform sampling]{\label{charex:fig:uTeam}\includegraphics[width=.33\textwidth]{part_02/semi_structured_annotation/ISWC_REX/random-team.pdf}}~
        \subfloat[Average computational time and F-measure over all datasets]{\label{charex:fig:runtime}\includegraphics[width=.33\textwidth]{part_02/semi_structured_annotation/ISWC_REX/time-F.pdf}}
\caption{Overall evaluation results of the extraction of pairs. Figures (a)-(h) show the average precision, recall and F-measure achieved the generated XPaths for the prominence-based and uniform sampling. The x-axis shows the number of examples and the number of sample pages retrieved in the format $|E|$/$|Q|$. Figure (i) shows the average computational time and the corresponding F-measures for different sizes of $|I|$.}
\label{charex:fig:overall-XPaths}
\end{figure*}

%\todo[inline]{RU: I think we need to define which f-measure was used here. AN: Done}
The results of our experiments on altering the number of examples used for learning are shown in Figures~\ref{charex:fig:pDirector}-\ref{charex:fig:uTeam}. 
Due to space limitations, we show the average results over all the pairs extraction by our wrapper induction approach for each of the domains.
The results achieved using the prominence-based sampling show the expected trend: on pages that use a consistent template (such as the director pages in \url{imdb.com}), our approach requires as few as around 70 pages for $|Q|$. 
Once this value is reached, REX can compute high-quality extraction rules and achieves an F-measure of 0.97 (see Figures~\ref{charex:fig:pDirector}).
For pages that change template based on the prominence of the entities they describe (like the actors' pages, see Figure~\ref{charex:fig:pActors}), our approach requires more training data to achieve a high F-measure.
The increase of F-measure is clearly due to an increase in precision, pointing to REX being able to better choose across different alternative XPaths when provided with more information.
The results of \url{goodreads.com} support our conjecture. 
With more training data, we get an increase in precision to up to 1 while the recall drops, leading to an overall F-measure of 0.89 for 40k examples.
In our worst-case scenario, we achieve an overall F-measure close to 0.6.
The lower value is clearly due to the inconsistent use of templates across the different pages in the subdomains.

\begin{table}[htb]
\centering
\caption{Average evaluation results using all available pairs as training data.} 
\begin{tabular}{lcccc}
\toprule
  &  P & R & F-measure & \# pages \\
\midrule
\url{dbo:director}  & 0.82 & 1.00 & 0.89 & 216\\
\url{dbo:starring}  & 0.86 & 1.00 & 0.90 & 316\\
\url{dbo:author}    & 0.94 & 0.85 & 0.86 & 217\\
\url{dbo:team}      & 0.32 & 0.43 & 0.35 & 656\\
\bottomrule
\end{tabular}
\label{tab:overall}
\end{table}


\begin{table}[htb!]
\centering
\begin{tabular}{lccccc}
\toprule
Property & \#Possible & \#Triples generated & \#Consistent & \#Correct & \#New   \\
 & triples   & by AlfREX& triples & triples & triples  \\
\midrule 
\url{dbo:author}$^{-1}$   & 54 & 32 & 32 & 22 & 22 \\% (67.7\%)\\
\url{dbo:author}          & 83 & 83 & 69 & 54 & 54 \\%(77.0\%)\\
\midrule 
\url{dbo:team}$^{-1}$     & 2  & 1  & 1  & 0  & 0  \\%(0\%)\\
\url{dbo:team}            & 30 & 55 & 42 & 19 & 13\\ %90 & 50 & ? & ? \\%(31.0\%) \\
\midrule 
\url{dbo:starring}$^{-1}$ & 40 & 99 & 83 & 35 & 34 \\%(42.7\%)\\
\url{dbo:starring}        & 70 & 70 & 44 & 33 & 32 \\%(73.9\%)\\
\midrule
\url{dbo:director}        & 61 & 56 & 52 & 41 & 41 \\%(78.8\%)\\
\bottomrule
\end{tabular} 
\caption{Triples generated by 100 randomly sampled pages, number of possible triples generated by using gold standard rules}
\label{tab:rdfeval}
\end{table}
The results based on the uniform sampling strategy reveal another trait of REX. 
As expected, the coverage achieved using uniform sampling is clearly smaller in all cases. 
%Still, when a small number of examples $|E|$ is provided, the uniform sampling returns better results than the prominence-based sampling (compare results with $|E| = 10k$ on Figures~\ref{charex:fig:pDirector} and~\ref{charex:fig:uDirector}, Figures~\ref{charex:fig:pActors} and~\ref{charex:fig:uActors}, Figures~\ref{charex:fig:pAuthor} and~\ref{charex:fig:uAuthor} as well as Figures~\ref{charex:fig:pTeam} and~\ref{charex:fig:uTeam}) in all settings.  
%This is simply due to the examples selected by the uniform sampling better reflecting the different types of templates in the dataset.
%The main drawback of the uniform sampling strategy is that the instances it selects generally contain a larger percentage of erroneous resources than the data selected by the prominence-based sampling strategy.
%The effect of this larger percentage of erroneous resources increases when making use of larger sizes of uniformly %sampled examples $E$ if the fraction of pages with noisy data in $Q$ increases.
%With a larger percentage of noise in the training data, REX is then more prone to learn biased and sub-optimal XPath expressions, leading to the overall decrease of the F-measure of our system.
%The proportion of erroneous pages in the training data is yet different from dataset to dataset, leading to different patterns combing about across our experiments. For example, extracting teams already leads to more errors in the uniform sampling when $|E| = 20k$ while the precision remains superior for $|E| = 20k$ when extracting author data from \url{goodreads.com}. 
%Our observations led us to the following conclusions: When running (1) on a manually curated knowledge base with only a small amount of erroneous data in the knowledge base or (2) on a small number of examples $|E|$, the uniform distribution is to be preferred.
%However, when running on an automatically extracted knowledge base or on a large set of examples, the prominence-based distribution should be used.
%An exact quantification of this behavior remains part of our future work.
The results achieved with all the training data available clearly show the importance of sampling (see Table~\ref{tab:overall}). 
While one could conjecture that using all data for training would be beneficial for our approach, the F-measures achieved by using all the data suggest that sampling can be beneficial for the extraction, especially when the web pages do not follow a rigid template (e.g., in \url{esnpfc.com}) or when the data in the knowledge base is noisy. 
Overall, our results suggest that our approach is accurate, also for pages where entities with different prominence are assigned variable templates as in \url{imdb.com} actors. 
If multiple occurrences of the same value are present in the same page (as in the case of books, actors and directors), our algorithm is able to detect the most stable one.
Moreover, our approach seems robust against noisy labels, even when there are many false positive in the page (e.g., book author pages that include many links to different books by the same author).
An important feature of our approach is that it can obtain accurate XPaths even by learning from a very small fraction of pages. 
For example, in our experiments on up to 40k pages, our approach learned XPath expressions from only 0.5\% to 1.16\% of $|W|$.
Still, for very noisy domains with an inconsistent use of templates, our approach can lead to less accurate extraction rules.
%, as shown by our results on the third dataset.
%Yet, manually crafting XPath expressions to extract these data is a very tedious task.


\subsubsection{Runtime Performance}
We evaluated the runtime performance of our approach by using 40k examples and the prominence-based distribution while altering the size of $I$.
As expected, setting $|I|$ to a low value (e.g.,~1) leads to less rules being generated and thus to an overall better runtime performance (see~Figure~\ref{charex:fig:runtime}).
By setting $I$ to a low value, REX can be used to get a quick overview of possible extraction results, a
characteristic of our system that could result beneficial for end users.
Yet, it also leads to worse overall F-measures.
Setting $I$ to a higher value (e.g., 15) leads to a more thorough (i.e., more time-demanding) analysis of the websites and thus to better results. 
Still overall, our approach scales quasi linearly and requires on average less than thirty seconds to learn wrappers out of existing data even for $|I|=20$.

\subsubsection{Quality of RDF Output}
To check the quality of the RDF we generated, we manually checked the triples extracted from each property of our three domains.
Each triple was checked by at least two annotators, which reached a significant Kohen's kappa score of 0.88 overall.
On \texttt{goodreads.com} we achieved a precision of 75.24\%.
While we achieve a precision of 78.85\% when extraction directors from \emph{imdb.com} and of 75\% on \texttt{starring}, the extraction of \texttt{starring}$^{-1}$ proves more tedious (precision = 42.17\%).
As expected, the data extracted from \emph{espnfc.com} has a low precision of 44.19\%.
The results on \texttt{starring}$^{-1}$ are due to the fact that several actors can star in a movie while assuming other roles. 
Thus, our extraction framework often overgenerates triples and produces false positives (e.g., directors are often included).
The results on \emph{espnfc.com} are clearly due to the templates not being used correctly.
Still, our results clearly show the potential of our approach, as 60.68\% of the triples we extracted are both correct and novel.
%Our results suggest that several sources of errors led to the average correctness of the triples we generate being around 64\% as shown in Table~\ref{tab:rdfeval}. 
%The first source of error (approximately 20\%) was that some extraction results were semantically wrong, i.e., the disambiguation was correct but the extracted data was incorrect.
%\todo[inline]{RU:Actually unknown resources (not in DBpedia) that could not be checked by consistency checking but went through AGDISTIS led to semantically incorrect triples and thereby provide 20\% error. So disambiguation was only 50\% the reason of 36\% error rate.}
%However, the main source of errors was the mapping of extracted strings to resources, which remains the most difficult task when extracting RDF from the Web. These wrong mappings are mostly due to the small number of strings being available for concurrent disambiguation (most often only two), leading to the right candidate pairs of resources being difficult to select.
%While we used a state-of-the-art approach for disambiguating the strings return by our wrappers, the evaluation of the RDF extracted suggests that disambiguating with this small number of resources often leads to resources with a higher prominence being preferred over the intended resources. For example, cities such as ``Milwall'' are often returned instead of less known soccer clubs such as ``Milwall FC''. Our approach can yet capture such errors due to the axioms learned from the knowledge base and discard many of these erroneous triples. In this particular example, the range of \texttt{dbo:team} being \texttt{dbo:SportsTeam} (which is disjoint from \texttt{dbo:City}) leads to this triple being excluded. Still, the overall accuracy value of the RDF extraction being similar to that achieved when disambiguating resources from small portions of text~\cite{Usbeck2013} hints towards our approach lacking context information to be able to disambiguate. A dedicated disambiguation which would take into consideration the textual context in which the strings are found is thus required to improve this aspect of REX. Devising such an approach will be part of our future work. 

%\todo[inline]{Lorenz, Saleem add the errors you noticed below please}
%\begin{itemize}
%\item kappa = 0.88
%\item espnfc errors: cities have been disambiguated instead of the soccer club, e.g. 'Milwall' vs. 'Milwall FC' since the club was named 'Milwall' on \url{espnfc}. Many players of teams in minor leagues have no correct name information, e.g. only a surname like 'Samuel' which complicates URI disambiguation and leads to false positives. The only team triple belonging to the soccer club 'Querétaro' was match on the city named the same.
%\item imdb errors: The main resoan for error while extracting actors and the movies they played in was, that the semantic was the page was not taken into consideration. The chosen subdomain \url{http://www.imdb.com/name} belongs not only to actors but also to directos, cutters and make-up artists. The extracted persons and movies belonged together but not in the sense of an actor starring in a movie.
%\item goodreads error: Mainly the XPath for author A extracts the first book on the page and the first author that can be found in the list of books of author A. If the first book was solely written by the author A, the first book will not have any author information in the list of authors. If then the second book was written by the author A and another author B the XPath extracts author B since also at the second book author A is omitted.
%\item overall: we noticed most of the errors during the disambiguation phase. especially the differences in encoding of DBpedia and our Domains led to most errors during the disambiguation phase.
%\end{itemize}
%AN: I added this to the text
%\begin{itemize}
% \item In Table~\ref{tab:overall} we show precision, recall and F$_1$-measure of the learned XPaths. We also report the average number of pages found $Q$.
% \item Overall the tecnique is accurate, also for variable templates (Director - Starring). If multiple occurences of the same value
%   is present in the page, the algorithm can find the most stable one.
% \item The tecnique is resistent to noisy labels, even when there are many false positive in the page (Author). In books' pages there
%   are many links to different books of the same author.
% \item Golden standard manually crafted, but in some cases hard to define. Team : 
% \begin{itemize}
%    \item Players' pages the team that the player has played with, is in different parts of the page and sometimes the data is in contradiction
%    \item Teams' pages same problem
% \end{itemize}
% \item In Figure~\ref{charex:fig:overall-XPaths} we show the quality of the learned XPaths considering as input a given number of pairs.
% \item Bigger is the set $Q$ better are the scores
% \item We could obtain really accurate XPaths learning from a small fraction of pages (from 0.5\% to 1.16\%)
%\end{itemize}
%\todo[inline]{describe results of table 3}
%\todo[inline]{RU: maybe add the percentage of new triples against the possible triples to convince reviewers that we are able to extract new data}

%\begin{table*}[htb!]
%\centering
%\begin{tabular}{lccccc}
%\toprule
%Property & \#Possible & \#Triples generated & \#Consistent & \#Correct & \#New   \\
% & triples   & by AlfREX& triples & triples & triples  \\
%\midrule 
%\url{dbo:author}$^{-1}$   & 54 & 32 & 30 & 21 & 21 \\% (67.7\%)\\
%\url{dbo:author}          & 83 & 83 & 81 & 58 & 57 \\%(77.0\%)\\
%\midrule 
%\url{dbo:team}$^{-1}$     & 2  & 1  & 0  & 0  & 0  \\%(0\%)\\
%\url{dbo:team}            & 30 & 90 & 60 & 3  & 1\\ %90  \\
%\midrule 
%\url{dbo:starring}$^{-1}$ & 40 & 99 & 94 & 34 & 34 \\%(42.7\%)\\
%\url{dbo:starring}        & 70 & 70 & 65 & 35 & 34 \\%(73.9\%)\\
%\midrule
%\url{dbo:director}        & 61 & 56 & 52 & 36 & 36 \\%(78.8\%)\\
%\bottomrule
%\end{tabular} 
%\caption{NEW AGDISTIS: Triples generated by 100 randomly sampled pages, number of possible triples generated by using gold standard rules}
%\label{tab:rdfeval}
%\end{table*}
\section{Related Work}
\label{sec:related}
To the best of our knowledge, no open-source framework covers the complete functionality of the REX framework. 
%While our approach is mainly related to approaches for the extraction of RDF data from the Web, 
REX relies internally on URI disambiguation and data validation based on automatically extracted axioms~\cite{Buehmann2012}. 
These are both areas of research with a wide of body of publications. 
Especially, several approaches to URI disambiguation based on graphs~\cite{AIDA,Usbeck2014} and statistical information from text~\cite{spotlight} have been developed recently. 
%Most disambiguation approaches try to optimize an objective function which maps each input string to a resource while taking into account the similarity between the input string and the resource, the a-priori probability of the resource being correct as well as the coherence between the returned resources. 
The extraction of axioms from knowledge based using statistical information~\cite{Buehmann2012,pattern_enrichment} as also flourished over the last years. 
The main idea underlying these approaches is to use instance knowledge from knowledge bases without expressive schemas to compute the axioms which underlie the said knowledge bases. 
We refer the reader to the publications above for an overview of these two research areas.

REX is mainly related to wrapper induction. 
Early approaches to learning web wrappers were mostly supervised (see, e.g.,~\cite{Hogue:2005:TAU:1060745.1060762,flesca2004web}). 
These systems were provided with annotated pages out of which they infer extraction rules that allow extracting data from other unlabeled pages with the same structure as the annotated pages). 
For example~\cite{Hogue:2005:TAU:1060745.1060762} presents \emph{Tresher}, a system that allows non-technical end-users to teach their browser how to extract data from the Web. 
%The approach relies on labeled web pages and a distance function between DOM trees to generate extraction templates based on user annotations.
%While remaining able to generate high-precision wrappers thanks to the human supervision, 
Supervised approaches were yet deemed costly due to the human labor necessary to annotate the input web pages. 
Unsupervised wrapper induction methods have thus been explored~\cite{DBLP:journals/aai/CrescenziM08,exalg} to reduce the annotation costs. 
However, the absence of a supervision often lead these systems to produce wrappers of accuracy not suitable for production level usage.
Novel approaches thus aim to minimize the annotation costs while keeping a high precision.
For example, the approach presented in~\cite{Dalvi:2011:AWL:1938545.1938547} relies on the availability of a knowledge base in the form of dictionaries and regular expressions to automatically obtain training data. 
%They present an inference algorithm to learn wrappers in presence of noise in the input data but neglect any issue that could arise from the presence of biased samples.
Recently, \cite{Crescenzi2013}~describes a supervised framework that is able to profit from crowd-provided training data. 
The learning algorithm controls the cost of the crowd sourcing campaign w.r.t. quality of the output wrapper.
However, these novel approaches do not target the generated of RDF data.
%miss the opportunities related to existence of \emph{Linked Data}, and the semantic consistency of the extracted data is out of their scope of interest.

%In order to accomplish the vision of the Semantic Web~\cite{Gentile2013} presents an approach for learning web wrappers learning that exploit \emph{Linked Data} as a training data source for their wrapper induction framework. Using instances and corresponding properties from the  LOD Cloud, they combine an unsupervised annotation of web pages followed by a standard wrapping process~\cite{Hao2011}. However, the process they adopt consists of a variety of manual steps and is thus very costly.

%Manifold approaches have been developed to address the problem of extracting structured data from web pages. For example, \cite{Hogue:2005:TAU:1060745.1060762} presents Tresher, a system that allows non-technical end-users to teach their browser how to extract data from the Web. The approach relies on labeled web pages and a distance function between DOM trees to generate extraction templates based on user annotations. 

Linked Data has been used to learn wrappers to extract RDF from the Web in recent years. 
For example,~\cite{Gentile2013} exploits Linked Data as a training data to find instances of given classes such as universities and extract the attributes of these instances while relying on the supervised wrapper induction approach presented in~\cite{Hao2011}. However, they require a manual exploration of the Linked Data sources to generate their training data, which leads to a considerable amount of manual effort.
%
% Linking the Deep Web to the Linked Data Web (DEIMOS)
The {\sc Deimos} project~\cite{conf/aaaiss/ParundekarKA10} is similar to REX, as it aims at bringing to the Semantic Web the data that are published through the rest of the Web. 
%As in REX, it uses automatic wrapper inference techniques to extract data from pages. 
However, it focuses on the pages behind web forms.
%; it strongly relies on the properties exposed by the form fields to access the pages, which are mapped into involved semantic descriptions of the sources.
% http://www.isi.edu/integration/papers/parundekar10-sss.pdf 
% DBLP:conf/aaaiss/ParundekarKA10,
% DEIMOS discover sources by using semantic tagging
% template based wrapper induction... AutoFeed
% DBLP:journals/tods/SuWL09 Citare ODE di Lovchosky
%
% OntoMiner: Bootstrapping and Populating Ontologies From Domain Speciﬁc Websites
% http://www.public.asu.edu/~hdavulcu/VLDB-WS03.pdf
% DBLP:journals/ijwgs/DavulcuVN05, heuristics approach
%
% Ontology-Driven Information Extraction with OntoSyphon
% document driven...multidomains
OntoSyphon~\cite{DBLP:journals/ws/McDowellC08} operates in an ``ontology-driven'' manner: taking any ontology as input, OntoSyphon uses the ontology to specify web searches that identify possible semantic instances, relations, and taxonomic information, in an unsupervised manner. However, the approach makes use of extraction patterns that work for textual documents rather than structured web pages. %, which are the focus of our proposal. 
To the best of our knowledge, none of the existing approaches covers all steps that are required to extract consistent RDF from the Web. 
Especially, only~\cite{conf/aaaiss/ParundekarKA10} is able to generate RDF but does not check it for consistency.
In contrast, REX is the first approach that is scalable, low-cost, accurate and can generate consistent RDF. 
%A comparison of existing technologies with REX (see Table~\ref{tab:comparison}) shows that none of these approaches covers all steps that are required to extract consistent RDF from the Web. 
%
% Enriching Ontology for Deep Web Search; focused on web searches
%
% Automatic Generation of Ontology from the Deep Web
%
% Ontology-Based Deep Web Data Sources Selection
% Solo sulla selezione delle sorgenti?
%
% An Unsupervised Approach for Acquiring Ontologies and RDF Data from Online Life Science Databases (Uses structured data, not really relevant but good to point out)
% http://userpages.uni-koblenz.de/~staab/Research/Publications/2010/MSReswc10.pdf
% Perform wrapper induction. Learn full ontologies from HTML templates

%\todo[inline]{Rotate table by 90°}

%\begin{table}[h]
%\centering
%\begin{tabular}{cccccc}
%\toprule
%                                    & Web-      & Low-      & Accuracy  & RDF    & Consistency  \\
%                                    & scale     & cost     &           &        &   \\
%\midrule
%\cite{exalg}                            & \cross    & \cross    & \cross    & \cross & \cross \\
%\cite{DBLP:journals/aai/CrescenziM08}   & \tick     & \tick     & \tick     & \cross & \cross \\
%\cite{Crescenzi2013}                    & \tick     & \cross    & \tick     & \cross & \cross \\
%\cite{Dalvi:2011:AWL:1938545.1938547}   & \tick     & \tick     & \tick     & \cross & \cross \\
%\cite{Gentile2013}                      & \tick     & \cross    & \tick     & \cross & \cross\\
%\cite{Hao2011}                          & \tick     & \cross    & \tick     & \cross & \cross\\
%\cite{Hogue:2005:TAU:1060745.1060762}   & \tick     & \cross    & \tick     & \cross & \cross \\
%\cite{DBLP:conf/ijcai/KushmerickWD97}   & \cross    & \cross    & $\emptyset$     & \cross & \cross \\
%\cite{DBLP:journals/ws/McDowellC08}     & \tick     & \tick     & \cross    & \cross & \tick \\
%\cite{Muslea:2001:HWI:608606.608666}    & \cross    & \tick     & \tick     & \cross & \cross \\
%\cite{conf/aaaiss/ParundekarKA10}       & \tick     & \tick     & $\emptyset$ & \tick & \cross \\
%\cite{Soderland:1999:LIE:309497.309510} & \tick     & \cross    & \cross     &\cross  & \cross \\
%REX                                     & \tick     & \tick     & \tick     & \tick  & \tick \\
%\bottomrule
%\end{tabular}
%\caption{Comparison of web wrapper induction approaches. Approaches are considered scalable to the Web if they can deal with whole web domains. The costs are considered low if no human supervision is needed. The accuracy is high if the approach can achieve F-measures beyond 90\%. The RDF criterion is fulfilled if the approach can return RDF data while the consistency column is ticked if the RDF generated is consistent with the ontology of the input knowledge base. $\emptyset$ means no evaluation was done by the authors.}   
%\label{tab:comparison}
%\end{table}

\section{Conclusions}
\label{sec:conclusions}
%\todo[inline]{RU: repetition of "to the best of our knowledge". AN: fixed}
In this paper we presented the first framework for the consistent extraction of RDF from templates Web pages. 
REX is available as open source\footnote{\url{http://rex.aksw.org}} Java implementation in an easily extendable fashion.
Our framework uses the LOD Cloud as source for training data that are used to learn web wrappers. 
The output of these wrappers is used to generate RDF by the means of a URI disambiguation step as well as a data validation step.
We studied several sampling strategies and how they affect the F-measure achieved.
%, and we provided suggestions on how to choose the best sampling strategy. 
Our overall results show that although we can extract subject-object pairs with a high accuracy from well-templated websites, a lot of work still needs to be done in the area of grounding these strings into an existing ontology.
One solution to this problem might be to use more context information during the disambiguation step.
%Furthermore, we did not consider certain dimensions of data that can influence the quality of the extracted data. For example, we implicitly assumed all triples to be time-independent (which is a basic assumption underlying RDF). Yet, when extracting data from the Web, taking time into consideration can be of central importance.   
%Time can be a problem due to websites contain current players and knowledge bases contain historical data, thus leading to the induced wrappers being very noisy 
Moreover, more sophisticated approaches can be used for crawling websites offering structured navigation paths towards target pages~\cite{DBLP:conf/webist/BlancoCM05}. 
By these means, we should be able to eradicate some of the sources of error in our extraction process. 
%These strategies can be used for crawling the corpus from which we extract the RDF and can be combined easily with the work presented herein.
%Our approach can be extended to use negative examples generated automatically.
Our approach can be further improved by combining it with crowdsourcing-based approaches for wrapper induction such as ALFRED~\cite{Crescenzi2013} or by learning more expressive wrappers.
We thus regard this framework as a basis for populating the Web of Data using Web pages by professional end-users.
%to check the content of knowledge bases by (1) using a sample of the knowledge bases, (2) extracting RDF, (3) checking the most informative triples that are in $K$ but for which we have the least confidence (using the crowd).

%Combine Alfred with refinement operators to enable it to generate more complex classes of rules (IMDB Actors page)
%Use output of AlfREX as input for Alfred

%Carry out better disambiguation by simply extracting more information on the resources where we are not sure
%\begin{wrapfigure}[2]{r}{0.18\textwidth}
% \vspace{-8mm}
% \includegraphics[width=0.18\textwidth]{esf.pdf}
%\end{wrapfigure}
%\textbf{Acknowledgments.} Parts of this work were supported by the ESF and the Free State of Saxony. We thank M.~Speicher \& M.~R\"oder.
%\todo[inline]{How do we engage the wrapper breakage problem?}
 
%\bibliographystyle{abbrv}
%\bibliography{sigproc}  
%\balancecolumns
 
%\end{document}

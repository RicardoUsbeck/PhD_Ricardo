\chapter{Synopsis}


\graffito{This chapter concludes the thesis and points to future research directions. It is based on the observations from all the author's thesis-relevant publications~\cite{rex,qasurvey, pool-foundations-lod,n3, gerbil_dev_2015,CETUS_2015, combiningLDandIR, HAWK_CLEF_2015, agdistis_ecai,agdistis_iswc, agdistisdemo,HAWK_NLIWOD_2015, hawk_2015, gerbil_demo_2015, GERBIL}}



The main aim of this thesis was to address three research challenges in the Semantic Web, namely the Semantic, Evaluation and Information Gap. In In our approaches, we focused on a combination of statistical methods on Linked Data. 
In this chapter, we first give a brief summary of the approaches and methodologies we developed to address each of these three challenges while referencing the associated research contributions.
Hereby, we collect the most import conclusions and discoveries resulting from our entanglement with these challenges.
Subsequently, future research avenues based on the afore presented conclusions are detailed.
 
\section{Summary}
\subsection*{Semantic Gap}

The overall goal is the development of an efficient information access leveraging natural language.
This should enable the access to complex data for end users.
However, such an approach demands novel solutions to extract high-quality, machine-readable semantic data from structured and semi-structured data sources. 
In the following, we present our solutions to these requirements.

First, we presented CETUS---a pattern based type extraction that can be used as baseline for other approaches.
Both versions---CETUS$_{YAGO}$ and CETUS$_{FOX}$---were explained in detail.
We showed how CETUS$_{YAGO}$ uses label matching to determine a super-type for automatically generated classes while CETUS$_{FOX}$ is based on one of the various, existing entity typing frameworks.
After the initial publication, CETUS participated in the Open Knowledge Extraction challenge~\cite{okechallenge}.
CETUS outperformed two other approaches~\cite{fred_typing,oak_sheffield} by achieving a  micro resp. macro F-measure(F1) of 0.47 resp. 0.45, therewith winning task 2~\cite{okechallenge}.	

Next, this thesis presented AGDISTIS, a novel approach for named entity disambiguation that combines the scalable \ac{HITS} algorithm and breadth-first search with linguistic heuristics.
This approach outperforms the state-of-the-art algorithms TagMe 2, AIDA and DBpedia Spotlight while remaining quadratic in its time complexity. 
Moreover, our evaluation suggests that while the approach performs well in and of itself, it can benefit from being presented with more linguistic information such as surface forms. 
Furthermore, we presented the demo of AGDISTIS for three different languages on three different DBpedia-based knowledge bases.
Here,  we emphasize that our works led to the development of the first \ac{NED} system for the Chinese language based on Linked Data.
AGDISTIS is integrated into FOX~\cite{FOX}, an ensemble learning based \ac{NER}~framework which can be found at \url{http://fox.aksw.org}. 
Furthermore, AGDISTIS is integrated in to the entity evaluation platform GERBIL~\cite{GERBIL}, see \url{http://gerbil.aksw.org/gerbil/}.
Furthermore, we provided a \ac{NED} API serving 4.648.056 calls for the English and 329.358 calls for the German endpoint while the Chinese endpoint was rarely used.


Finally, we tackled semi-structured data sources by aiming to extract \ac{RDF} from highly-templated websites. 
REX is the first framework for the consistent extraction of \ac{RDF} from templated Web pages. 
This approach is available as open-source Java implementation and can be extended at will.
Our framework uses the \ac{LOD} Cloud as source for training data that are used to learn Web wrappers. 
The output of these wrappers is used to generate \ac{RDF} by the means of a URI disambiguation step.
Our framework also includes a consistency layer with which the data extracted by the wrappers can be checked for logical consistency~\cite{buhmann2012}.
Our overall results show that we can extract subject-object pairs with a high accuracy from well-templated websites.

Overall, our results suggest that this work was able to advance the state of the art by providing novel approaches for the scalable extraction of high-quality RDF data from unstructured text as well as for the enhancement of knowledge bases. 
The data resulting from these approaches can be (and partly is) used to feed the knowledge bases underlying our own hybrid \ac{QA} system HAWK. 


\subsection*{Evaluation Gap}

As mentioned before, the need for high-quality and efficient RDF data extraction from any unstructured, semi- or structured data sources triggered the development of a multitude of approaches. 
Gaining an overview to decide on a specific framework for a specific use case or to analyze in detail the behavior of  a system towards a certain kind of data became increasingly difficult.
Hence, we wanted to bridge the Evaluation Gap to  support developers, decision makers and end users.
First, we presented a novel, unstructured dataset collection to leverage reuse and and comparability dubbed $\mbox{N}^3$~\cite{n3}. 
Second, we described and evaluated GERBIL, a platform for the evaluation of annotation frameworks. With GERBIL, we aim to push annotation system developers to better quality and wider use of their frameworks.

%%% N3
The dataset collection $\mbox{N}^3$ can be used for \ac{NER}  and \ac{NED}  benchmarks as well as algorithm tuning.
We compared our corpora with two already known datasets and showed the advantages of our datasets.
The usability of these corpora for \ac{NED}  benchmark has been proven in several other publications~\cite{agdistis_iswc,GER+13,GERBIL}.
Especially, $\mbox{N}^3$ aims at providing a structured and standardized language resource for unstructured texts, enabling semantic querying.
Our datasets link \ac{NLP}  algorithms to the Semantic Web by leveraging the power of NIF and \ac{LD} .

%%%GERBIL
One of the main contributions of GERBIL includes the provision of persistent URLs for reproducibility and archiving.
GERBIL implements a generic adapter for external datasets as well as a generic interface to integrate remote annotator systems.
The datasets available for evaluation in the previous benchmarking platforms for annotation was extended to 19 datasets. 
Moreover, 15 annotators were added to the platform. 
The evaluation of our framework by contributors suggests that adding an annotator to GERBIL demands 1 to 2 hours of work. 
Hence, while keeping the implementation effort previously required to evaluate on a single dataset, we allow developers to evaluate on (currently) 19 times more datasets.
The presented, Web-based front-end allows for several use cases enabling laymen and expert users to perform informed  comparisons of semantic annotation systems.

The persistent URIs enhances the long term quotation in the field of information extraction.
GERBIL is not just a new framework wrapping existing technology. 
In comparison to earlier frameworks, GERBIL extends the state-of-the-art benchmarks by the capability of considering the influence of NIL attributes.
Additionally, we are able to deal with datasets and annotators that link to different knowledge bases.
As mentioned in Section~\ref{sec:currentNumbersForGERBIL}, GERBIL provides additional value to the community by storing more than 1.824 comparable experiments since its release. 

We efficiently targeted the Evaluation Gap w.r.t. the domain of semantic annotation systems by providing means to easily reuse datasets as well as whole experiments and publish their results in a machine-readable way.

\subsection*{Information Gap}

As mentioned previously, the increasing amount of data, both in the Data and the Document Web is a bilateral risk. 
To efficiently access and understand this constantly growing data volume one can benefit from the machine-readable and straightforward semantics underlying the structured RDF knowledge source. 

To provide easy and natural language-based access to end-users and laymen, we introduced HAWK, a hybrid \ac{QA} system for the Web of Data.
We analyzed its performance against \ac{QALD}-4 and \ac{QALD}-5 datasets.
By using a generic approach to generate SPARQL queries from predicate-argument structures,   HAWK is able to achieve up to 0.68 F-measure on the hybrid \ac{QALD}-4 benchmark using an optimal ranker.
The system is also able to achieve an F-measure of up to 0.3 on the \ac{QALD}-5 training benchmark using bucket-based ranking.
Especially, HAWK achieves a F-measure of up to 0.35 based on boolean and entity-centric questions over the combined \ac{QALD}-5 benchmark.
HAWK served already more than 600 questions from real users generating organic traffic.

Introducing the HAWK framework enables users ask natural language questions without the need to understand the underlying data scheme or a complex query language. 
Thus, laymen can benefit from both the Data and the Document Web to gain answers on complex questions. 



\section{Conclusion and Future Work}
\subsection*{Semantic Gap}

By investigating and developing solutions to enhance un- or semi-structured data sources by means of the presented approaches, our work shed light on several insights and further raised some highly interesting questions that serve as seed to our future research. 

We see CETUS and AGDISTIS as first steps in a larger research agenda.
The field of type identification on unstructured text is still not well-researched and pattern-based approaches miss the opportunity to benefit from existing string expansions methods and existing knowledge bases like Wordnet~\cite{wordnet}.


Since AGDISTIS is agnostic of the underlying \ac{KB} and language-independent, it can profit from growing \ac{KB}s as well as multilingual Linked Data.
However, moving from one \ac{KB} to another is still bound to adapting a set of parameters and fine-tuning scores. 
Without the provision of automatic means and gold standards for the target domain it will always be a cumbersome endeavor. 
In the future, we will thus extend AGDISTIS by using different underlying \ac{KB}s and even more domain-specific datasets.
Moreover, we will implement a sliding-window-based extension of AGDISTIS to account for large amounts of entities per document.
A novel avenue of research would be combining AGDISTIS with topic modeling~\cite{Blei:2003:LDA:944919.944937}. 
Preliminary experiments in this direction show that we can improve the F-measure of our approach by at least 1\% on all datasets.
Furthermore, we intend to look for larger, more domain-specific and even more insightful disambiguation datasets to refine and test AGDISTIS.
Also, a deeper evaluation of ontology structures towards disambiguation accuracies is needed.

%%%%REX
While the former mentioned approaches worked well on unstructured data we also gained insights from REX, our scalable approach towards Web-scale RDF extraction from templated websites. 
With REX, we studied several sampling strategies and how they affect the F-measure achieved.
A lot of work still needs to be done in the area of grounding these strings into an existing ontology.
One solution to this problem might be to use more context information during the disambiguation step.
Moreover, more sophisticated approaches can be used for crawling websites offering structured navigation paths towards target pages~\cite{DBLP:conf/webist/BlancoCM05}. 
By these means, we should be able to eradicate some of the sources of error in our extraction process. 
Our approach can be further improved by combining it with crowdsourcing-based approaches for wrapper induction such as ALFRED~\cite{Crescenzi2013} or by learning more expressive wrappers.


Regarding our novel approaches---CETUS, AGDISTIS and REX-- we aim to develop a new paradigm for realizing \ac{NLP} services which employ community-generated, multilingual and evolving Linked Open Data background knowledge.
%Other than most work, which mainly uses statistics and heuristics, we aim to truly exploit the graph structure and semantics of the background knowledge.
We thus regard these frameworks as a basis for populating the Web of Data using Web pages by professional end-users.


\subsection*{Evaluation Gap}

$\mbox{N}^3$ has shown to be useful to a number of approaches and especially for testing within the GERBIL framework. 
However, the enlargement of the corpora and the improvement of their quality through the community are major issues that need to be worked on. 
Gaining insights into the behavior of semantic annotation frameworks requires a larger dataset.
Furthermore, converting more existing datasets to NIF and re-bundle them in order to provide even more insightful NER and NED benchmarks is the next step of research in this area.


Next, while developing GERBIL, we discovered several short-comings in the formal model underlying previous benchmarking frameworks which we aim to explore in the future. 
For example, the formal specification underlying current benchmarking frameworks for annotation does not allow using the scores assigned by the annotators for their results. 
To address this problem, we aim to develop/implement novel measures into GERBIL that make use of scores (e.g., Emerging Entities~\cite{Hoffart:2014:DEE:2566486.2568003}).
Moreover, partial results are not considered within the evaluation. 
For example, during the disambiguation task, named entities without Wikipedia URIs are not considered. 
This has a significant impact of the number of true and false positives and thus on the performance of some systems.
Furthermore, certain tasks seem to be too coarse. 
Those issues are addressed in current GERBIL releases, see Section~\ref{sec:currentNumbersForGERBIL}. 
In future work, we also aim to provide a new theory for evaluating annotation systems and display this information in the GERBIL interface.
Additionally, we will perform a detailed analysis of the benchmarking platform's log files and draw conclusions for future development.
Also, we plan to provide information about the point in time since when an annotator is stable, i.e., the algorithm underlying the Web service has not changed.
GERBIL has sparked the idea for the successful Horizon 2020 proposal HOBBIT and will influence the central architecture underlying the HOBBIT Big Linked Data Benchmarking Platform.\footnote{\url{http://project-hobbit.eu/}} 


\subsection*{Information Gap}

Providing natural language access to hybrid data sources with HAWK led to an in-depth error analysis and thus captured future open research directions.
Currently, HAWK faces several limitations, such as not always being able to capture the exact semantics of certain resources due to missing dictionary entries (e.g., vice-president), the ability to use \texttt{FILTER} and SPARQL aggregation functions (\texttt{FILTER (?high > 100)}). 
The most important open issue lies in finding the correct ranking approach to map a predicate-argument tree to a possible interpretation. 
So far, our experiments reveal that the mere finding of the right features for this endeavor remains a challenging problem. 

Furthermore, we aim to integrate HAWK in domain-specific information systems where the more specialized context will most probably lead to higher F-measures. 
Additionally, we will assess the impact of full-text components over regular LD components for \ac{QA} and partake in the creation of larger benchmarks such as \ac{QALD}-6,
Another aim is to develop HAWK towards multilingual, schema-agnostic queries.
Moreover, negations within questions and improved ranking will  be considered. 
Finally, several components of the HAWK pipeline are computationally very complex. 
Thus, finding more time-efficient algorithms for these steps will be addressed in future works.
HAWK will be a starting point for the Eurostars projects DIESEL\footnote{\url{http://diesel-project.eu/}} and QAMEL\footnote{\url{http://qamel.eu/}} towards implementing novel search paradigms on large industrial data as well as mobile devices.

\bigskip

Overall, this thesis is the first stepping stone towards a longer and exciting research direction. 
Developing novel, scalable and adaptable \ac{QA} technologies using any kind of existing data will be at the center of considerations.
Providing means for an data-agnostic information access for every kind of user is about to be one driving factors of future research. 
We are convinced by the ongoing digitization of smart devices and sensors to be on the border to the next information access paradigm.  
For example, personal data-driven assistant systems for researchers, mechanics, tourists and even crisis managers lie  ahead and point out thrilling fields of future work.
Those systems need a thorough understanding of semantics from data as well as from human natural language. 
By creating the basic technologies, we foresee the development of dialog-driven \ac{QA} systems for faster, more precise and high-quality knowledge access.
Moreover, this research will continue in the direction of intelligent assistants leveraging non-static data, user context information to provide a self-learning and adaptive system environment.
Developing means for intelligent assistants will also consist of dialogue-based interaction models capturing sentiments, emotions and other sensor meta-data.
Overall, we aim at realizing the vision of self-improving semantic systems boosting the effectiveness of personal and corporate information access.
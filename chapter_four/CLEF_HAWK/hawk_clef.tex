
%\documentclass{llncs}

%\usepackage[english]{babel}
%\usepackage[utf8]{inputenc}
%\usepackage[hyphenbreaks]{breakurl}
%\usepackage[hyphens]{url}
%\usepackage{amsmath,amssymb}
%\usepackage{graphicx}
%\usepackage{float}%figure environment
%\usepackage[table,xcdraw]{xcolor}
%\usepackage{todonotes}
%\usepackage[disable]{todonotes}
%\usepackage{booktabs}
%\usepackage{wrapfig}
%\usepackage{multirow}
%\usepackage{tikz}
%\usetikzlibrary{positioning}
%\usepackage{endnotes}
%\usepackage[ruled,vlined]{algorithm2e}
%\usepackage{caption}
%\usepackage{comment}
%\usepackage{times}
%\begin{document}

\chapter{HAWK@QALD5 -- Trying to answer hybrid questions with various simple ranking techniques}
%\author{
%Ricardo Usbeck\inst{1} \and 
%Axel-Cyrille Ngonga Ngomo\inst{1}
%}

%\institute{
%University of Leipzig, Germany\\\email{\{usbeck,ngonga\}@informatik.uni-leipzig.de}
%}

%\maketitle

%\begin{abstract}
The growing amount of data available in the Document Web as well as in the Linked Data Web has lead to an information gap.
Information needed to answer complex questions might often require full-text data as well as Linked Data.
Thus, HAWK combines unstructured and structured data sources.   

In this article, we introduce HAWK, a novel entity search approach for hybrid question answering based on combining Linked Data and textual data.
In this article, we compare three ranking mechanism and evaluate their performance on the QALD-5 challenge. 
Finally, we identify the weak points of our current version of HAWK and give directions for future development.
%\end{abstract}


\section{Introduction}
The fifth challenge on Question Answering over Linked Data (QALD-5)\footnote{\url{http://www.sc.cit-ec.uni-bielefeld.de/qald/}} has introduced and extended a novel benchmark dataset for hybrid question answering.
In this paper, we present our framework HAWK, the second best performing system w.r.t. hybrid question answering. 
The need for this challenge becomes obvious by comparing the growing amount of data, in the Document Web and the Linked Data Web, which introduces an information gap, i.e., a considerable number of complex questions can only be answered by using hybrid question answering approaches, which can find and combine information stored in both structured and textual data sources~\cite{combiningLDandIR}.

In this paper, we will outline the single steps performed by HAWK to answer hybrid questions in Section~\ref{chaclef:sec:method}.
Section~\ref{chaclef:sec:discussion} discusses problems and opportunities within the current implementation.
Finally, we conclude in Section~\ref{chaclef:sec:conclusion} and explain future research directions.
The source code of HAWK, benchmark data, a link to the demo, further information as well as evaluation results can be retrieved from the project website \url{http://aksw.org/Projects/hawk}.


\section{Method}
\label{chaclef:sec:method}

In the following, we will shortly describe the 8-step, modular pipeline of HAWK absed on the following running example: \texttt{Which anti-apartheid activist was born in Mvezo?}
For more information please have a look at the full method description~\cite{hawk_2015}.


\textbf{1. Segmentation and Part-of-Speech (POS)-Tagging.} 
To be generic with respect to the language of the input question, HAWK uses a modular system that is able of tokenizing even languages without clear separation like Chinese.
For English input questions our system relies on the \emph{clearNLP}~\cite{choi2011getting}-framework which provides a.o. a white space tokenizer, POS-tagger and transition-based dependency parsing.
Afterwards, this framework annotates each token with its POS-tag which will be later used to identify possible semantic annotations. 
POS-tagging on the running example will result in the following: \texttt{Which(WDT) anti-apartheid(JJ) activist(NN) was(VBD) born(VBN) in(IN) Mvezo(NNP)?}

\textbf{2. Entity Annotation.} 
Next, our approach identifies named entities and tries to link them to the underlying knowledge base. 
The QALD-5 challenge relies on the DBpedia~\cite{jl_2014/swj_dbpedia} as source for structured information in the form of Linked Data. 
For recognizing and linking named entities HAWK's default annotator is FOX~\cite{FOX}, a federated knowledge extraction framework based on Ensemble Learning. 
FOX has shown to outperform other entity annotation systems on the QALD 4 benchmark data~\cite{hawk_2015}.
An optimal annotator would annotate our running example \texttt{Mvezo} with \url{http://dbpedia.org/resource/Mvezo}.

\textbf{3. Dependency Parsing and Noun Phrase Detection.}
Subsequently, in order to capture linguistic and semantic relations, HAWK parses the query using dependency parsing and semantic role labeling~\cite{choi2011getting}.
The dependency parser generates a predicate-argument tree based on the preprocessed question.
%@Axel: warum nochmal predicate argument trees?
Afterwards, HAWK identifies noun phrases, i.e., semantically meaningful word groups not yet recognized by the entity annotation system, using the result of the POS tagging step. 
Input tokens are combined following manually-crafted linguistic heuristics based on POS-tag sequences derived from the QALD 5 benchmark questions. %, and their POS-tag is changed to \texttt{CNN}.
Two domain experts implemented the deduced POS-tag sequences and safeguarded the quality of this algorithm w.r.t. the QA pipeline f-measure. 
The full algorithm can be found in our source code repository at \url{https://github.com/aksw/hawk}.
Here, the \texttt{anti-apartheid activist} would be detected as noun phrase.

\textbf{4. Linguistic Pruning.}
HAWK has now captured entities from a given knowledge base, noun phrases as well as the semantic structure of the sentence. 
Still, this structure contains tokens that are meaningless for retrieving the target information or even introduce noise in the process.
Thus, HAWK prunes nodes from the predicate-argument tree based on their POS-tags, e.g., deleting all \texttt{DET} nodes, interrogative phrases such as \texttt{Give me} or \texttt{List}, and auxiliary tokens such as \texttt{did}.
The linguistically pruned dependency tree with combined noun phrases for our running example would only contain \texttt{born} as a root node with two children, namely \texttt{anti-apartheid activist} and \url{http://dbpedia.org/resource/Mvezo}\footnote{Thereafter, we assume \texttt{dbo} stands for \url{http://dbpedia.org/ontology}, \texttt{dbr} for \url{http://dbpedia.org/resource/} and \texttt{rdfs} for \url{http://www.w3.org/2000/01/rdf-schema\#} }.

\textbf{5. Semantic Annotation.}
Now, the tree structure contains only semantically meaningful (combined) token and entities, i.e, individuals from the underlying knowledge base. 
To map the remaining token to properties and classes from the target knowledge base and its underlying ontology, our framework uses information about possible verbalizations of ontology concepts and leverages a fuzzy string search.
These verbalizations are based on both \texttt{rdfs:label} information from the ontology itself and (if available) verbalization information contained in lexica, in our case in the existing DBpedia English lexicon\footnote{\url{https://github.com/cunger/lemon.dbpedia}}.
After this step, either a node is annotated with a class, property or individual from the target knowledge base or it causes a full-text lookup in the targeted Document Web parts.
With respect to the running example \texttt{born} would be annotated with the properties \texttt{dbo:birthPlace} and \texttt{dbo:birthDate}.

\textbf{6. Generating hybrid SPARQL Queries.}
Given an (partly) annotated predicate argument, HAWK generates hybrid SPARQL queries.
It uses an Apache Jena FUSEKI\footnote{\url{http://jena.apache.org/documentation/serving_data/}} server, which implements the full-text search predicate \texttt{text:query} on a-priori defined literals. 
Those literals are basically mappings of textual information to a certain individual URI from a target knowledge, i.e., an implicit enrichment of structured knowledge from unstructured data. 

Especially, our framework HAWK indexes a-priori the following information per individual: \texttt{dbo:abstract}, \texttt{rdfs:label}, \texttt{dbpedia:redirect} and \texttt{dc:subject} to capture document based information.
Those information are then retrieved by using the \texttt{text:query} predicate by using exact matches or fuzzy matches on each non-stopword token of an indexed field.

The generation of SPARQL triple pattern is based on a pre-order walk to reflect the empirical observation that i) related information are situated close to each other in the tree and ii) information are more restrictive from left to right.
This breadth-first search visits each node and generates several \emph{possible triple patterns} based on the number of annotations and the POS-tag itself. 
Using this approach allows HAWK to be independent of SPARQL templates and to work on natural language input of any length and complexity.
Each pattern contains at least one variable from a pre-defined set of variables, i.e., \texttt{?proj} for the resource projection variable, \texttt{?const} for resources covering 
constraints related to the projection variable as well as a variety of variables for predicates to inspect the surrounding of elements in the knowledge base graph. 
During this process, each iteration of the traversal appends the generated patterns to each of the already existing SPARQL queries. 
This combinatorial effort results in covering every possible SPARQL graph pattern given the predicate-argument tree.
Amongst others, HAWK generates for the running example the following three hybrid SPARQL queries:
\begin{enumerate}
\item \texttt{SELECT ?proj  \{?proj text:query 'anti-apartheid activist'.\\ ?proj dbo:birthPlace dbr:Mvezo.\}}
\item \texttt{SELECT ?proj  \{?proj text:query 'anti-apartheid activist'.\\ ?proj dbo:birthDate dbr:Mvezo.\}}
\item \texttt{SELECT ?proj  \{?proj text:query 'anti-apartheid activist'.\\ ?const dbo:birthPlace ?proj.\}}
\end{enumerate}

\textbf{7. Semantic Pruning of SPARQL Queries.}
Covering each possible SPARQL graph pattern with the above algorithm results in a large number of generated SPARQL queries.
To effectively handle this large set of queries and reduce the computational effort, HAWK implements various methods for pruning. 
For example, HAWK can safely assume that unconnected query graphs, missing projection variables and cyclic SPARQL triple pattern lead to empty or semantically meaningless results and thus discards those queries.
Thus, the semantic pruning discards query number two from above because it violates the range restriction of the \texttt{dbo:birthDate} predicate.
Although semantic pruning drastically reduces the amount of queries, it often does not result in only one query. HAWK thus requires a final ranking step before sending the SPARQL query to the target triple store.

\textbf{8. Ranking and Cardinality.}
For the QALD-5 challenge, we extended the basic ranking implementation of HAWK by one additional ranking methods:
\begin{itemize}
\item \textbf{Optimal Ranking.} To ensure, we are able to generate hybrid SPARQL queries capable of answering the benchmark questions, the optimal ranker returns always those hybrid SPARQL queries which lead to a maximum f-measure.  
    Obviously, the optimal ranking can only be used if the answers are know, i.e., HAWK operates on the training data.
    This ranking functions allows to determine the parts of the hybrid question answering pipeline which do not perform. 
\item \textbf{Feature-based Ranking.} The second ranking method is based on supervised training over the gold standard answer set from the QALD-4 benchmark.
    In the \emph{training phase}, all generated queries are run against the underlying SPARQL endpoint. 
    Comparing the results to the gold standard answer set, HAWK stores all queries resulting with the same high F-measure.
    Afterwards the stored queries are used to calculate an average feature vector comprising simple features mimicking a centroid-based cosine ranking.
    HAWK's ranking calculation comprises the following components:
    \begin{itemize}
    \item \textbf{NR\_OF\_TERMS} calculates the number of nodes used to form the full-text query part as described above.
    \item \textbf{NR\_OF\_CONSTRAINTS} counts the amount of triple patterns per SPARQL query.
    \item \textbf{NR\_OF\_TYPES} sums the amount of patterns of the form \texttt{?var rdf:type cls}.
    \item \textbf{PREDICATES} generates a vector containing an entry for each predicate used in the SPARQL query.
    \end{itemize}
    While running the \emph{test phase} of HAWK, the cosine similarity between each SPARQL query using the above mentioned features and the average feature vector of training queries is calculated.
\item \textbf{Overlap-based Ranking.} The novel overlap-based ranking accounts for the intuition that the same result set can be generated by several hybrid SPARQL queries. 
    Thus, this ranker, although computationally highly expensive, executes every hybrid SPARQL query and the resulting answer sets are then stored into hashed buckets. 
    Finally, the ranker computes how many queries produced a certain answer set. 
    The answer set with the highest number is than returned.
\end{itemize}
Moreover, HAWK determines the target cardinality $x$, i.e., \texttt{LIMIT $x$}, of each query using the indicated cardinality of the first seen POS-tag, e.g., the POS-tag \texttt{NNS} demands the plural while \texttt{NN} demands the singular case and thus leads to different \texttt{x}.
An optimal ranking will reveal that the winning SPARQL query for our running example is \texttt{SELECT ?proj  \{?proj text:query 'anti-apartheid activist'.\\ ?proj dbo:birthPlace dbr:Mvezo.\}}.
%More detailed descriptions of each step of HAWK's pipeline are available at~\cite{hawk_2015}.
%Moreover, HAWK's modular architecture allows to exchange single modules in the pipeline to account.


\section{Evaluation and Discussion}
\label{chaclef:sec:discussion}
\sloppy
The QALD-5 benchmark has a training and a test dataset for question answering containing a subset of hybrid benchmark questions.
In particular, HAWK is currently only capable of answering questions demanding a single or a set of URIs from a target knowledge base. 
Moreover, questions depending on Yago ontology\footnote{\url{http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/}} types cannot be answered.
Thus, the QALD-5 dataset contains 26 training, respectively 8 test questions, suitable for the current implementation of HAWK.
Using the online available evaluation tool\footnote{\url{http://greententacle.techfak.uni-bielefeld.de/~cunger/qald/index.php?x=evaltool&q=5}}, Table~\ref{tab:eval_qald5} shows the results for the training and test dataset as well as well as for all three ranking approaches.
Please note, that for the feature-based ranker the training data was taken from QALD-4.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\centering
\caption{Results of the QALD-5 challenge for different ranking algorithms. Number in brackets show the amount of generated answers, i.e., HAWK outputs at least one result set.}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{@{}lrrr@{}}
\toprule
Dataset           & \multicolumn{1}{l}{Optimal Ranking} & \multicolumn{1}{l}{Feature-based Ranking} & \multicolumn{1}{l}{Overlap-based Ranking} \\ \midrule
QALD-5 - training & 0.30 (15 out of 26)                 & 0.06 (22 out of 26)                       & 0.08 (22 out of 26)                       \\
QALD-5 - test     & 0.1 (1 out of 10)                   & 0.10 (3 out of 10)                        & 0.10 (3 out of 10)                        \\ \bottomrule
\end{tabular}
}
\label{tab:eval_qald5}
\end{table}

As can be seen in Table~\ref{tab:eval_qald5}, the implemented ranking function do not reach an optimal ranking implementation.
Moreover, each implementation are not able to discard whole queries, i.e., they are currently not aware of the possibility that the answer could not have been retrieved at all while the optimal ranker discards questions with incorrect answer sets. 
This does not effect the global f-measure but results in an high computational effort and might lead users to wrong conclusions. 

Table~\ref{tab:eval_qal5_detail} shows a detailed description of achieved measures per question and algorithm. 
Delving deeper into the results, we color-coded the single mistakes of the HAWK system.
(Yellow) indicates that the ranking algorithms are not able to retrieve the correct SPARQL query out of the set of generated SPARQL queries. 
(Orange) cells point out that one ranking algorithm performs worse than the other. 
(Red) indicates the inability to retrieve correct answer sets.
That is, HAWK is able to generate a set of SPARQL queries but amongst them none retrieves a correct answer set.
Finally, (Blue) rows describe questions where HAWK is unable to generate at least one SPARQL query.
Thus, those questions semantics cannot be captured by the system yet due to missing surface forms for individuals, classes and properties or missing indexed full-text information.
Especially, in the test dataset all three ranking algorithms are only able to generate one correct answer while the feature-based and the overlap-based ranker perform differently on the train dataset.
To experience the run times for single queries please visit our demo at \url{http://hawk.aksw.org}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[htb]
\centering
\caption{Detailed results of HAWK at the QALD-5 challenge.}
\scriptsize
\resizebox{\textwidth}{!}{ 
\begin{tabular}{@{}lp{0.4\linewidth}lllllllll@{}}
\toprule
         &                                                                                                             & \multicolumn{3}{l}{{\bf Feature-based Ranking}}     & \multicolumn{3}{l}{{\bf Overlap-based Ranking}}                                   & \multicolumn{3}{l}{{\bf Optimal Ranking}}                                      \\ \midrule
{\bf ID} & {\bf Question}                                                                                              & {\bf Recall}                & {\bf Precision}              & {\bf F1}             & {\bf Recall}              & {\bf Precision}           & {\bf F1}          & {\bf Recall}             & {\bf Precision}          & {\bf F1}         \\\midrule
301      & Who was vice-president under the president who authorized atomic weapons against Japan during World War II? & \cellcolor[HTML]{FFFE65}0   & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & 1                        & 1                        & 1                        \\
303      & Which anti-apartheid activist was born in Mvezo?                                                            & \cellcolor[HTML]{F8A102}0   & \cellcolor[HTML]{F8A102}0    & \cellcolor[HTML]{F8A102}0    & 1                         & 0.5                       & 0.67                      & 1                        & 0.5                      & 0.67                     \\
305      & Which recipients of the Victoria Cross died in the Battle of Arnhem?                                        & \cellcolor[HTML]{F8A102}0   & \cellcolor[HTML]{F8A102}0    & \cellcolor[HTML]{F8A102}0    & 0.5                       & 0.33                      & 0.4                       & 0.5                      & 0.5                      & 0.5                      \\
306      & Where did the first man in space die?                                                                       & \cellcolor[HTML]{FFFE65}0   & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & 1                        & 1                        & 1                        \\
308      & Which members of the Wu-Tang Clan took their stage name from a movie?                                       & \cellcolor[HTML]{F8A102}0.5 & \cellcolor[HTML]{F8A102}0.08 & \cellcolor[HTML]{F8A102}0.14 & 1                         & 0.17                      & 0.29                      & 0.5                      & 0.5                      & 0.5                      \\
309      & Which writers had influenced the philosopher that refused a Nobel Prize?                                    & \cellcolor[HTML]{FFFE65}0   & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & 1                        & 1                        & 1                        \\
311      & Who composed the music for the film that depicts the early life of Jane Austin?                             & \cellcolor[HTML]{FE0000}   & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\
314      & Which horses did The Long Fellow ride?                                                                      & \cellcolor[HTML]{FFFE65}0   & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & 0.86                     & 1                        & 0.92                     \\
315      & Of the people that died of radiation in Los Alamos, whose death was an accident?                            & \cellcolor[HTML]{FFFE65}0   & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & 0.5                      & 1                        & 0.67                     \\
316      & Which building owned by the Bank of America was featured in the TV series MegaStructures?                   & \cellcolor[HTML]{FE0000}   & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\
317      & Which buildings in art deco style did Shreve, Lamb and Harmon design?                                       & 1                           & 0.33                         & 0.5                          & \cellcolor[HTML]{F8A102}0 & \cellcolor[HTML]{F8A102}0 & \cellcolor[HTML]{F8A102}0 & 1                        & 0.33                     & 0.5                      \\
318      & Which birds are protected under the National Parks and Wildlife Act?                                        & 0.67                        & 0.67                         & 0.67                         & 0.67                      & 0.67                      & 0.67                      & 0.67                     & 0.67                     & 0.67                     \\
319      & Which country did the first known photographer of snowflakes come from?                                     & \cellcolor[HTML]{FFFE65}0   & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & 1                        & 1                        & 1                        \\
320      & List all the battles commanded by the lover of Cleopatra.                                                   & \cellcolor[HTML]{FFFE65}0   & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & 0.23                     & 0.42                     & 0.29                     \\
322      & Which actress starring in the TV series Friends owns the production company Coquette Productions?           & \cellcolor[HTML]{FFFE65}0   & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & 1                        & 1                        & 1                        \\
323      & Gaborone is the capital of which country member of the African Union?                                       & 1                           & 1                            & 1                            & 1                         & 1                         & 1                         & 1                        & 1                        & 1                        \\
326      & For which movie did the daughter of Francis Ford Coppola receive an Oscar?                                  & \cellcolor[HTML]{FE0000}   & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\
327      & Which city does the first person to climb all 14 eight-thousanders come from?                               & \cellcolor[HTML]{BBDAFF}    & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} \\
328      & At which college did the only American actor that received the César Award study?                           & \cellcolor[HTML]{FE0000}   & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\
332      & What is the native city of Hollywood's highest-paid actress?                                                & \cellcolor[HTML]{BBDAFF}    & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} \\
333      & In which city does the former main presenter of the Xposé girls live?                                       & \cellcolor[HTML]{FE0000}   & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\
334      & Who plays Phileas Fogg in the adaptation of Around the World in 80 Days directed by Buzz Kulik?             & \cellcolor[HTML]{FFFE65}0   & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0    & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & \cellcolor[HTML]{FFFE65}0 & 1                        & 1                        & 1                        \\
335      & Who is the front man of the band that wrote Coffee \& TV?                                                   & \cellcolor[HTML]{FE0000}   & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\
336      & Which Chinese-speaking country is a former Portguese colony?                                                & \cellcolor[HTML]{BBDAFF}    & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} \\
337      & What is the largest city in the county in which Faulkner spent most of his life?                            & \cellcolor[HTML]{BBDAFF}    & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} \\
340      & A landmark of which city is the home of the Mona Lisa?                                                      & \cellcolor[HTML]{FE0000}   & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\
\hline
51       & Where was the "Father of Singapore" born?                                                                   & \cellcolor[HTML]{BBDAFF}    & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} \\
52       & Which Secretary of State was significantly involved in the United States' dominance of the Caribbean?       & \cellcolor[HTML]{FE0000}   & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\
53       & Who is the architect of the tallest building in Japan?                                                      & \cellcolor[HTML]{BBDAFF}    & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} \\
55       & In which city where Charlie Chaplin's half brothers born?                                                   & \cellcolor[HTML]{BBDAFF}    & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} \\
56       & Which German mathematicians were members of the von Braun rocket group?                                     & \cellcolor[HTML]{FE0000}   & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000}    & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} & \cellcolor[HTML]{FE0000} \\
57       & Which writers converted to Islam?                                                                           & \cellcolor[HTML]{BBDAFF}    & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} \\
59       & Which movie by the Coen brothers stars John Turturro in the role of a New York City playwright?             & 1.00                        & 1.00                         & 1.00                         & 1.00                      & 1.00                      & 1.00                      & 1.00                     & 1.00                     & 1.00                     \\
60       & Which of the volcanoes that erupted in 1550 is still active?                                                & \cellcolor[HTML]{BBDAFF}    & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}     & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF}  & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} & \cellcolor[HTML]{BBDAFF} \\ \bottomrule
\end{tabular}
}
\label{tab:eval_qal5_detail}

\end{table}

\section{Conclusion}
\label{chaclef:sec:conclusion}

In this paper we presented HAWK, the first hybrid QA system for the Web of Data. We showed that by using a generic approach to generate SPARQL queries from predicate-argument structures, HAWK is able to achieve an F-measure of up to 0.3 on the QALD-5 training benchmark. 
These results demand novel approaches for capturing the semantic meanings of natural language questions with hybrid SPARQL queries.
Our work on HAWK, however, revealed several other open research questions, of which the most important lies in finding the correct ranking approach to map a predicate-argument tree to a possible interpretation. 
So far, our experiments reveal that the mere finding of the right features for this endeavor remains a challenging problem. 
Finally, several components of the HAWK pipeline are computationally very complex.
Finding more time-efficient algorithms for these steps will be addressed in future work.

%\vspace{.2cm}


%\bibliographystyle{abbrv}
%\bibliography{llncs}

%\end{document}
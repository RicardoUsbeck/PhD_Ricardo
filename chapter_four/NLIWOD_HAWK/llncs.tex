
\documentclass{llncs}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[hyphenbreaks]{breakurl}
\usepackage[hyphens]{url}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{float}%figure environment
\usepackage[table,xcdraw]{xcolor}
\usepackage{todonotes}
%\usepackage[disable]{todonotes}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows}
%\usepackage{endnotes}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{caption}
\usepackage{comment}
\usepackage{times}
\begin{document}

\title{Answering Boolean Hybrid Questions with HAWK}
\author{
Ricardo Usbeck\inst{1} \and 
Erik KÃ¶rner\inst{1} \and
Axel-Cyrille Ngonga Ngomo\inst{1}
}

\institute{
University of Leipzig, Germany\\\email{\{usbeck,ngonga\}@informatik.uni-leipzig.de}
}

\maketitle

\begin{abstract}
%The increasing amount of structured and unstructured data creates new opportunities and challenges for satisfying user information needs.
%Shifting towards conversational search and retrieval of knowledge highlights the need for natural language based information systems.
The decentral architecture behind the Web has led to pieces of information being distributed across data sources with varying structure. Hence, answering complex questions often requires combining information from structured and unstructured data sources.   
We present an extension for HAWK, a novel search approach for Hybrid Question Answering based on combining Linked Data and textual data.
Especially, we introduce our preliminary approach towards answering ASK queries. This approach relies a classification heuristic next to the existing predicate-argument representations of questions to derive equivalent combinations of SPARQL query fragments and text queries. These are executed so as to integrate the results of the text queries into SPARQL and thus generate a formal interpretation of the query.
%
%is based on a full-fledged search pipeline which comprises entity annotation, POS tagging, dependency parsing and SPARQL query generation, pruning and ranking.
%We present a thorough evaluation of the framework, including an analysis of the influence of entity annotation tools on the generation process of the hybrid queries and a study of the overall accuracy of the system. 
%accurate, hybrid SPARQL queries for answering information needs which are able to cover knowledge which is not or cannot be modeled in structured ontologies.
Our results show that these developments lead to HAWK achieving 0.74 F-measure on the ASK queries contained in the Question Answering over Linked Data (QALD-5) hybrid query benchmark assuming an given optimal ranking function. % and 0.60 F-measure on the test dataset.
\end{abstract}


\section{Introduction}
%\todo[inline]{@Future I: One of the worst intros ever. Redo this!}
%\todo[inline]{Rewrote it, hope you like it. Greetings to the past! Your future I}
Recent advances in question answering (QA) over Linked Data provide end users with more and more sophisticated tools for querying linked data by allowing users to express their information need in natural language~\cite{SINA_WebSemantic,tbsl,pythia}. 
This allows access to the wealth of structured data available on the current Semantic Web also to non-experts. However, a lot of information is still available only in textual form, both on the Document Web and in the form of labels and abstracts in Linked Data sources~\cite{rdflivenews}.
Therefore, a considerable number of questions can only be answered by using hybrid question answering approaches, which  can find and combine information stored in both structured and textual data sources~\cite{combiningLDandIR}.

In this paper, we extend HAWK, our hybrid QA system with the capabilities to answer also boolean question, i.e., generate SPARQL queries using \texttt{ASK}.  %we present HAWK, the (to best of our knowledge) first full-fledged hybrid QA framework for entity search over Linked Data and textual data. 
%To the best of our knowledge, this is the first hybrid question answering system, combining information from structured and unstructured data.
Given a textual input query $q$, HAWK implements the following pipeline, which comprises 1) input segmentation 2) part-of-speech tagging, 3) detecting entities and 4) noun phrases in $q$, 5) dependency parsing and 6) applying linguistic pruning heuristics for an in-depth analysis of the natural language input. 
The results of these steps is a predicate-argument graph annotated with resources from the Linked Data Web. HAWK then 7) assigns semantic meaning, i.e., properties and classes, to nodes and 8) generates basic triple patterns for each component of the input query with respect to a multitude of features. 
This deductive linking of triples results in a set of SPARQL queries containing text operators as well as triple patterns.
In order to reduce operational costs, 9) HAWK discards queries using several rules, e.g., by discarding not connected query graphs.
Here, 10) HAWK applies a simple heuristic to classify \texttt{SELECT} and \texttt{ASK} queries and 11) modifies the generated SPARQL queries 12) respectively determines the cardinality of the \texttt{SELECT} query.
Finally, 13) queries are ranked using various ranking methods.
The current version of HAWK works on DBpedia as Linked Data source as well as Wikipedia abstracts for full-text information. 
For the sake of this paper, we will assume the implementation of an optimal ranking function for the generated SPARQL queries since we focus on capturing the semantics of the input question in this paper. 

%We evaluate HAWK on the QALD-4 benchmark\footnote{\url{http://www.sc.cit-ec.uni-bielefeld.de/qald/}} for hybrid question answering. 
%As data source it uses a triple store containing DBpedia 3.9 as well as full-text information based on the Wikipedia abstracts of all loaded resources.
%The evaluation sections reports on micro F-measure, and analyzes the influence of different entity annotation systems on the overall question answering performance.

%\todo[inline]{@Axel: Contribs}
Our main contributions can be summarized as follows: (1) We present HAWK, the first QA framework tackling hybrid question answering. (2) HAWK analyses input questions and classifies them according to a simple heuristic. (3) The modular architecture of HAWK allows simple exchanging of pipeline parts to enhance testing and deployment, e.g., novel ranking or classification functions. (4) Our evaluation suggests that HAWK is able to achieve F-measures of 0.74 on QALD-5 hybrid test set.

The rest of the paper is structured as follows:
In Section~\ref{sec:relatedwork}, we briefly describe related work and in Section~\ref{sec:method} we detail HAWK's architecture.
HAWK's performance and the influence of entity annotation systems is evaluated in Section~\ref{sec:evaluation}. 
Finally, we conclude in Section~\ref{sec:conclusion}. 
A demo as well as additional information can be found at our project home page \url{http://aksw.org/Projects/HAWK.html}.


\section{Related Work} 
\label{sec:relatedwork}
\input{relatedwork}

\section{Method}
\label{sec:method}
\begin{figure}[htb!]
\centering
\input{architecture}
\caption{Pipeline steps of HAWK.}
\label{fig:hawk_pipeline}
\end{figure}


In the following, we will shortly describe the 8-step, modular pipeline of HAWK via this running example: \texttt{Did Napoleon's first wife die in France?}
For more information please have a look at the full method description~\cite{HAWK_2015}.


\textbf{1. Input Segmentation.} 
To be generic with respect to the language of the input question, HAWK uses a modular system that is able of tokenizing even languages without clear separation like Chinese\footnote{\url{https://github.com/clir/clearnlp}}.
For English input questions our system relies on the \emph{clearNLP}~\cite{choi2011getting}-framework which provides a.o. a white space tokenizer, POS-tagger and transition-based dependency parsing.

\textbf{2. Part-of-Speech (POS)-Tagging.} 
HAWK annotates each token with its POS-tag which will be later used to identify possible semantic annotations. 
POS-tagging on the running example will result in the following: \texttt{Did(VBD) Napoleon(NNP) 's(POS) first(JJ) wife(NN) die(VB) in(IN) France(NNP) ?(.)}

\textbf{3. Entity Annotation.} 
Next, our approach identifies named entities and tries to link them to the underlying knowledge base. 
Most QA challenges, including QALD-5, rely on the DBpedia~\cite{jl_2014/swj_dbpedia} as source for structured information in the form of Linked Data. 
For recognizing and linking named entities HAWK's default annotator is FOX~\cite{FOX}, a federated knowledge extraction framework based on Ensemble Learning. 
%FOX has shown to outperform other entity annotation systems on the QALD 4 benchmark data~\cite{HAWK_2015}.
An optimal annotator would annotate our running example \texttt{Napoleon} with \url{http://dbpedia.org/resource/Napoleon} and \texttt{France} with \url{http://dbpedia.org/resource/France}.

\textbf{4. Noun Phrase Detection.}
HAWK identifies noun phrases, i.e., semantically meaningful word groups, i.e., real-world entities or concepts not captured by the underlying knowledge base, not yet recognized by the entity annotation system, using the result of the POS tagging step. 
Input tokens are combined following manually-crafted linguistic heuristics based on POS-tag sequences derived from the QALD 5 benchmark questions. %, and their POS-tag is changed to \texttt{CNN}.
Two domain experts implemented the deduced POS-tag sequences and safeguarded the quality of this algorithm w.r.t. the QA pipeline f-measure. 
%The full algorithm can be found in our source code repository at \url{https://github.com/aksw/hawk}.
Here, the \texttt{first wife} would be detected as noun phrase.

\textbf{5. Dependency Parsing.}

To capture linguistic and semantic relations, HAWK parses the query using dependency parsing and semantic role labeling~\cite{choi2011getting}.
The dependency parser generates a predicate-argument tree based on the preprocessed question.
%@Axel: warum nochmal predicate argument trees?

\textbf{6. Linguistic Pruning.}
Here, HAWK has captured entities from a given knowledge base, noun phrases as well as the semantic structure of the sentence. 
Still, this structure contains tokens that are meaningless for retrieving the target information or even introduce noise into the process.
Thus, HAWK prunes nodes from the predicate-argument tree based on their POS-tags, e.g., deleting all \texttt{DET} nodes, interrogative phrases such as \texttt{Give me} or \texttt{List}, and auxiliary tokens such as \texttt{did}.
The linguistically pruned dependency tree with combined noun phrases for our running example would only contain \texttt{born} as a root node with two children, namely \texttt{anti-apartheid activist} and \url{http://dbpedia.org/resource/Mvezo}.

\textbf{7. Semantic Annotation.}
Now, the tree structure contains only semantically meaningful (combined) token and entities, i.e, individuals from the underlying knowledge base. 
To map the remaining token to properties and classes from the target knowledge base and its underlying ontology, our framework uses information about possible verbalizations of ontology concepts and leverages a fuzzy string search.
These verbalizations are based on both \texttt{rdfs:label}\footnote{We assume \texttt{dbo} stands for \url{http://dbpedia.org/ontology}, \texttt{dbr} for \url{http://dbpedia.org/resource/}, \texttt{rdfs} for \url{http://www.w3.org/2000/01/rdf-schema#}, \texttt{dc} for \url{http://purl.org/dc/elements/1.1/} and \texttt{text} for \url{http://jena.apache.org/text#} } information from the ontology itself and (if available) verbalization information contained in lexica, in our case in the existing DBpedia English lexicon\footnote{\url{https://github.com/cunger/lemon.dbpedia}}.
After this step, either a node is annotated with a class, property or individual from the target knowledge base or it causes a full-text lookup in the targeted Document Web parts.
With respect to the running example \texttt{die} would be annotated with the properties \texttt{dbo:deathPlace} and \texttt{dbo:dbo:deathDate}.

\textbf{8. Generating hybrid SPARQL Queries.}
Given a (partly) annotated predicate argument, HAWK generates hybrid SPARQL queries.
It uses an Apache Jena FUSEKI\footnote{\url{http://jena.apache.org/documentation/serving_data/}} server, which implements the full-text search predicate \texttt{text:query} on a-priori defined literals. 
Those literals are basically mappings of textual information to a certain individual URI from a target knowledge, i.e., an implicit enrichment of structured knowledge from unstructured data. 

Especially, our framework HAWK indexes a-priori the following information per individual: \texttt{dbo:abstract}, \texttt{rdfs:label}, \texttt{dbo:redirect} and \texttt{dc:subject} to capture document based information.
This information is then retrieved by a special FUSEKI predicate (\texttt{text:query}) by using exact matches or fuzzy matches on each non-stopword token of an indexed field.

The generation of SPARQL triple pattern is based on a pre-order walk to reflect the empirical observation that i) related information is situated close to each other in the tree and ii) information is more restrictive from left to right.
This breadth-first search visits each node and generates several \emph{possible triple patterns} based on the number of annotations and the POS-tag itself. 
With this approach HAWK is independent of SPARQL templates and to work on natural language input of any length and complexity.
Each pattern contains at least one variable from a pre-defined set of variables, i.e., \texttt{?proj} for the resource projection variable, \texttt{?const} for resources covering 
constraints related to the projection variable as well as a variety of variables for predicates to inspect the surrounding of elements in the knowledge base graph. 
During this process, each iteration of the traversal appends the generated patterns to each of the already existing SPARQL queries. 
This combinatorial effort results in covering every possible SPARQL graph pattern given the predicate-argument tree.
Amongst others, HAWK generates for the running example the following three hybrid SPARQL queries:
\begin{enumerate}
\item \texttt{SELECT ?proj  \{?proj text:query 'first wife'.\\ ?proj dbo:deathPlace dbr:France.\\ ?proj ?pbridge dbr:Napoleon.\}}
\item \texttt{SELECT ?proj  \{?proj text:query 'first wife'.\\ ?proj dbo:deathDate dbr:France.\\ ?proj ?pbridge dbr:Napoleon.\}}
\item \texttt{SELECT ?proj  \{?proj text:query 'first wife'.\\ ?const pbridge dbr:France.\\ ?proj ?pbridge dbr:Napoleon.\}}
\end{enumerate}

\textbf{9. Semantic Pruning of SPARQL Queries.}
Covering each possible SPARQL graph pattern with the above algorithm results in a large number of generated SPARQL queries.
To effectively handle this large set of queries and reduce the computational effort, HAWK implements various methods for pruning. 
For example, it assumes that unconnected query graphs, missing projection variables and cyclic SPARQL triple pattern lead to empty or semantically meaningless results.
Thus, HAWK discards those queries.

In the running example, the semantic pruning discards query number two from above because it violates the range restriction of the \texttt{dbo:deathDate} predicate.
Although semantic pruning drastically reduces the amount of queries, it often does not result in only one query. 
Thus, a ranking of the remaining queries is applied before the best SPARQL query is send to the target triple store.

\textbf{10. Classification of ASK Queries.}
To decide whether the user intended a set of entities or a boolean answer as result, HAWK relies on a simple heuristic based on the first word, dubbed indicator word, of the query, see Table \ref{tab:indicator_words}. 
We tried using POS tags for the same purposes. However, experiments using POS tags failed due to missing semantics of POS-tags. 
Furthermore, we acknowledge that classifying questions based on word-level analysis is not language-independent. 
In the future, we will work on a language independent version of the module leveraging the dependency structure of the input question. 
Obviously, our running example is classified as \texttt{ASK} demanding question.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[htb!]
\centering
\caption{Indicator Word for classifying ASK queries in English questions.}
\label{tab:indicator_words}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Indicator Word (POS-tag)} & \textbf{Stem form}\\ \midrule
Do (VBP), Does (VBZ), Did (VBD) & do      \\
Is (VBZ), Are (VBP), Was (VBD)  & be        \\
Have (VBP), Has (VBZ), Had (VBD)& have     \\
 \bottomrule
\end{tabular}
\end{table} 

\textbf{11. Modify the SPARQL query.}
After classifying questions  and detecting the need for an \texttt{ASK} query, HAWK modifies the existing structure, i.e., changes the type of the SPARQL query by replacing the \texttt{SELECT} in the query with \texttt{ASK}.
Furthermore, HAWK skips the cardinality calculation due to \texttt{ASK} queries not requiring the \texttt{LIMIT} solution modifier.\footnote{\url{http://www.w3.org/TR/rdf-sparql-query/#ask}} 

With respect to our running example, the final query would be: 
\begin{enumerate}
\item \texttt{ASK \{?proj text:query 'first wife'.\\ ?proj dbo:deathPlace dbr:France.\\ ?proj ?pbridge dbr:Napoleon\}}
\end{enumerate}

\textbf{12. Cardinality.}
If HAWK classifies an input question as entity search related rather than demanding a boolean answer, we need to determine the target cardinality $x$, i.e., modify the solution modifier \texttt{LIMIT $x$}. 
The number of answers expected for a given query is indicated by cardinality of the first seen POS-tag, e.g., the POS-tag \texttt{NNS} demands the plural while \texttt{NN} demands the singular case and thus leads to different $x$.
That is, each plural indicating POS-tag will return 10 results by default rather than 1. 
In the future, we will use a machine learning-based algorithm to learn the correct number of $x \geq 1$.


\textbf{13. Ranking}
For the sake of this paper, we assume an optimal ranking algorithm to demonstrate the capabilities of HAWK to identify and generate the correct \texttt{ASK} query per input question rather than additionally introducing noise in the process of choosing the correct \texttt{ASK} amongst the generated SPARQL queries.
To ensure we are able to generate hybrid SPARQL queries capable of answering the benchmark questions, the optimal ranker returns always those hybrid SPARQL queries which lead to a maximum f-measure.  
Obviously, the optimal ranking can only be used if the answers are know, i.e., HAWK operates on the QALD 5 benchmark datasets containing the gold standard answer set.
This ranking functions allows to determine the parts of the hybrid question answering pipeline which do not perform well. 
An optimal ranking will reveal that the winning SPARQL queries for our running example are:
\begin{enumerate}
\item \texttt{ASK \{?proj text:query 'first wife'. \\?proj dbo:deathPlace dbr:France. \\?proj ?pbridge dbr:Napoleon\}}, 
\item \texttt{ASK \{?proj text:query ('first wife' AND 'Napoleon') .\\ ?proj dbo:deathPlace dbr:France.\}}.
\end{enumerate} 
%More detailed descriptions of each step of HAWK's pipeline are available at~\cite{HAWK_2015}.
%Moreover, HAWK's modular architecture allows to exchange single modules in the pipeline to account.


\section{Evaluation}
\label{sec:evaluation}
\sloppy
The QALD-5 \cite{qald5} benchmark has a training and a test dataset for question answering containing a subset of hybrid benchmark questions.
%In this paper, we focus on the evaluation of hybrid questions demading boolean answers.
Moreover, questions depending on Yago ontology\footnote{\url{http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/}} types cannot be answered.

The QALD-5 dataset contains 40 training, respectively 10 test questions for hybrid QA.
To increase the number of gold standard queries, we did not restrict ourselves to only hybrid, boolean questions but to boolean questions in general, not demanding aggregations, e.g., \texttt{FILTER}, and only DBpedia ontology types.
Thus, we used 27 questions for the evaluation from the combined dataset following the restrictions given above\footnote{\url{https://github.com/AKSW/hawk/blob/master/resources/qald-5_test_train.xml}}.
%Using the online available evaluation tool\footnote{\url{http://greententacle.techfak.uni-bielefeld.de/~cunger/qald/index.php?x=evaltool&q=5}}, Table~\ref{tab:eval_qald5} shows the results for the training and test dataset as well as well as for all three ranking approaches.
%Please note, that for the feature-based ranker the training data was taken from QALD-4.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
\centering
\caption{Results with and without the \texttt{ASK}-extension of HAWK.}
\label{tab:eval_ask}
\begin{tabular}{@{}lrr@{}}
\toprule
\multicolumn{1}{c}{{\bf Question Type}} & \multicolumn{1}{c}{{\bf Global F-measure}} & \multicolumn{1}{c}{{\bf Global F-measure with skipping}} \\ \midrule
Hybrid SELECT                           & 0.19                                       & 0.27                                                     \\
Hybrid ASK                              & 0.47                                       & 0.74                                                     \\
Hybrid SELECT+ASK                       & 0.24                                       & 0.35                                                     \\ \bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:eval_ask} details our results on the combined QALD 5 dataset using an optimal ranker approach. 
Our simple classification is able to decide in all cases for the correct command method w.r.t. the benchmark data.
The \emph{skipping} measure takes into account, iff HAWK does not generate any answer set, i.e., returns 'I do not know the answer'.
Overall, the novel implementation of the \texttt{ASK}-related modules improves the overall F-measure by more than $10\%$. With this F-measure, we outperform the winner of the QALD-5 challenge, which achieved an F-measure of 0.26. 
%Please note, that a baseline approach returning always true (21 out of 27) or false (6 out of 27) is less insightful, despite the fact the f-measure might be better. 
%In the future, we will use the generated SPARQL queries to explain the answer.
A demo of the framework can be found at \url{http://hawk.aksw.org}.



\section{Conclusion}
\label{sec:conclusion}
In this paper, we briefly introduced HAWK, a hybrid QA system for the Web of Data, and analysed its performance against the combined QALD 5 dataset using the new \texttt{ASK}-query module. 
We showed that by using a generic approach to generate SPARQL queries from predicate-argument structures, HAWK is able to achieve an F-measure of up to 0.35.
Currently, HAWK faces several limitations, such as not capturing the exact semantics due to missing dictionaries (e.g., vice-president), the ability to use \texttt{FILTER} and SPARQL aggregation functions (\texttt{FILTER (?high > 100)}) or compound questions. 
Besides, HAWK assumes error-free data sources and user input to focus on the underlying mechanisms that capture the input semantics.
Furthermore, the current version of HAWK is based on a frequently modified Java implementation whose performance can be experience in the demo. 
Our work on HAWK, however, revealed several other open research questions, of which the most important lies in finding the correct ranking approach to map a predicate-argument tree to a possible interpretation. 

\vspace{.2cm}

\textbf{Acknowledgments}
This work has been supported by the FP7 project GeoKnow (GA No. 318159), the BMWI project SAKE and the Eurostars projects DIESEL and QAMEL.

\bibliographystyle{abbrv}
\bibliography{llncs}

\end{document}